{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# User input\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# data = dataset\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m task_type_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250;43m                    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;43;03m                    Task type (int):\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;43;03m                    1. Regression\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;43;03m                    2. Classification\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;43;03m                    3. Clustering\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;43;03m                    \"\"\"\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m task_type_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClustering\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     21\u001b[0m task_type_str \u001b[38;5;241m=\u001b[39m task_type_dict[task_type_num]\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# User input\n",
    "\n",
    "# data = dataset\n",
    "\n",
    "task_type_num = int(input(\n",
    "                    \"\"\"\n",
    "                    Task type (int):\n",
    "                    1. Regression\n",
    "                    2. Classification\n",
    "                    3. Clustering\n",
    "                    \"\"\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "task_type_dict = {\n",
    "    1: \"Regression\",\n",
    "    2: \"Classification\",\n",
    "    3: \"Clustering\"\n",
    "}\n",
    "\n",
    "task_type_str = task_type_dict[task_type_num]\n",
    "print(f\"Selected task type is {task_type_str}\\n\")\n",
    "\n",
    "task_type = task_type_str.lower()\n",
    "\n",
    "if task_type_str in (\"Regression\", \"Classification\"):\n",
    "    label = input(\"Select the target column: \")\n",
    "\n",
    "print(f\"Traget column/label is {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "task_type_list = ['regression', 'classification', 'clustering']\n",
    "\n",
    "for task_type in task_type_list:\n",
    "    \n",
    "    if task_type == 'regression':\n",
    "        p\n",
    "\n",
    "    elif task_type == 'classification':\n",
    "        p\n",
    "\n",
    "    elif task_type == 'clustering':\n",
    "        p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **I. Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "\n",
    "from mealpy import FloatVar, StringVar, IntegerVar, BoolVar, MixedSetVar, Problem, ACOR, GA, PSO, SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    return pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     sex     bmi  children smoker     region      charges\n",
      "0   19  female  27.900         0    yes  southwest  16884.92400\n",
      "1   18    male  33.770         1     no  southeast   1725.55230\n",
      "2   28    male  33.000         3     no  southeast   4449.46200\n",
      "3   33    male  22.705         0     no  northwest  21984.47061\n",
      "4   32    male  28.880         0     no  northwest   3866.85520 \n",
      "\n",
      "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4   5            5.0           3.6            1.4           0.2  Iris-setosa \n",
      "\n",
      "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0           1    Male   19                  15                      39\n",
      "1           2    Male   21                  15                      81\n",
      "2           3  Female   20                  16                       6\n",
      "3           4  Female   23                  16                      77\n",
      "4           5  Female   31                  17                      40 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test load data\n",
    "\n",
    "for task_type in task_type_list:\n",
    "    \n",
    "    if task_type == 'regression':\n",
    "        df = load_data(\"./data/insurance.csv\")\n",
    "        df_reg = df\n",
    "    \n",
    "    elif task_type == 'classification':\n",
    "        df = load_data(\"./data/iris.csv\")\n",
    "        df_clf = df\n",
    "    \n",
    "    elif task_type == 'clustering':\n",
    "        df = load_data(\"./data/Mall_Customers.csv\")\n",
    "        df_cls = df\n",
    "\n",
    "    print(df.head(), '\\n')\n",
    "\n",
    "df_list = [df_reg, df_clf, df_cls]\n",
    "task_df_zip = zip(task_type_list, df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **III. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data, task_type, label=None):\n",
    "    # If there's no label input\n",
    "    if label is None:\n",
    "        label = data.columns[-1]\n",
    "\n",
    "    data = data_cleaning(data, task_type, label)\n",
    "    data = data_transformation(data, task_type, label)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before preprocessing:\n",
      "    age     sex     bmi  children smoker     region      charges\n",
      "0   19  female  27.900         0    yes  southwest  16884.92400\n",
      "1   18    male  33.770         1     no  southeast   1725.55230\n",
      "2   28    male  33.000         3     no  southeast   4449.46200\n",
      "3   33    male  22.705         0     no  northwest  21984.47061\n",
      "4   32    male  28.880         0     no  northwest   3866.85520 \n",
      "\n",
      "After preprocessing:\n",
      " [          age       sex       bmi  children    smoker    region\n",
      "0    0.347385  1.023333 -0.447469  0.766327 -0.342193  1.374625\n",
      "1    1.058097  1.023333  1.629846 -0.059748 -0.342193  1.374625\n",
      "2    1.768810 -0.977199  1.536196  1.592401 -0.342193  0.467087\n",
      "3    0.631670  1.023333  0.029292  0.766327 -0.342193  1.374625\n",
      "4   -0.079043  1.023333 -1.516776  1.592401 -0.342193  0.467087\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "949  1.200240 -0.977199  2.023174 -0.885822 -0.342193  0.467087\n",
      "950  0.205242  1.023333  1.217788  0.766327 -0.342193  0.467087\n",
      "951  1.129168 -0.977199  0.080373 -0.885822 -0.342193  1.374625\n",
      "952 -0.292256  1.023333 -0.498551 -0.059748 -0.342193  1.374625\n",
      "953  0.134171 -0.977199 -0.336793 -0.059748 -0.342193  0.467087\n",
      "\n",
      "[954 rows x 6 columns],           age       sex       bmi  children    smoker    region\n",
      "0   -0.150114  1.023333  2.809829  1.592401 -0.342193  0.467087\n",
      "1   -1.074040  1.023333  0.613324  2.418476 -0.342193 -1.347988\n",
      "2   -1.429396  1.023333 -0.226116 -0.885822 -0.342193  1.374625\n",
      "3   -0.860826 -0.977199  0.063346  1.592401 -0.342193 -0.440451\n",
      "4   -1.429396  1.023333  0.936840 -0.885822 -0.342193 -0.440451\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "234 -0.789755  1.023333 -1.279247  0.766327 -0.342193 -1.347988\n",
      "235 -1.500467  1.023333 -0.826324 -0.885822  2.922328 -1.347988\n",
      "236  0.631670  1.023333  0.128049  1.592401 -0.342193 -1.347988\n",
      "237 -0.150114 -0.977199 -0.617741 -0.885822  2.922328  0.467087\n",
      "238  1.413453  1.023333 -0.777796 -0.885822 -0.342193 -0.440451\n",
      "\n",
      "[239 rows x 6 columns], 74       7726.8540\n",
      "162     10450.5520\n",
      "603     16085.1275\n",
      "424      8968.3300\n",
      "408      6652.5288\n",
      "           ...    \n",
      "1172    11093.6229\n",
      "1227     7162.0122\n",
      "1266    10704.4700\n",
      "965      4746.3440\n",
      "1262     6770.1925\n",
      "Name: charges, Length: 954, dtype: float64, 660      6435.62370\n",
      "754     17128.42608\n",
      "487      1253.93600\n",
      "429     18804.75240\n",
      "559      1646.42970\n",
      "           ...     \n",
      "813      4428.88785\n",
      "157     15518.18025\n",
      "645     10141.13620\n",
      "750     19539.24300\n",
      "1015    12124.99240\n",
      "Name: charges, Length: 239, dtype: float64] \n",
      "\n",
      "\n",
      "Before preprocessing:\n",
      "    Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4   5            5.0           3.6            1.4           0.2  Iris-setosa \n",
      "\n",
      "After preprocessing:\n",
      " [     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "0        -1.721568     -0.324840      -1.347036     -1.320168\n",
      "1        -1.124492     -1.226129       0.414290      0.651867\n",
      "2         1.144395     -0.550162       0.584741      0.257460\n",
      "3        -1.124492      0.125805      -1.290219     -1.451638\n",
      "4        -0.408002     -1.226129       0.130206      0.125991\n",
      "..             ...           ...            ...           ...\n",
      "115      -1.124492      0.125805      -1.290219     -1.451638\n",
      "116      -1.363322      0.351127      -1.403853     -1.320168\n",
      "117      -0.408002      2.604352      -1.347036     -1.320168\n",
      "118       1.263810      0.125805       0.641558      0.388929\n",
      "119      -1.482738      0.125805      -1.290219     -1.320168\n",
      "\n",
      "[120 rows x 4 columns],     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "0       -1.721568     -0.099517      -1.403853     -1.320168\n",
      "1        0.308489     -0.099517       0.641558      0.783336\n",
      "2       -1.124492     -1.451452      -0.267513     -0.268416\n",
      "3       -1.005077     -1.676774      -0.267513     -0.268416\n",
      "4       -1.721568      0.351127      -1.403853     -1.320168\n",
      "5        0.547319      0.576450       0.527924      0.520398\n",
      "6       -1.482738      1.252417      -1.574303     -1.320168\n",
      "7       -0.527417      0.801772      -1.176585     -1.320168\n",
      "8        0.786149     -0.099517       0.812009      1.046275\n",
      "9       -0.527417     -0.099517       0.414290      0.388929\n",
      "10       1.741470     -0.324840       1.436996      0.783336\n",
      "11       1.263810      0.125805       0.755192      1.440682\n",
      "12       0.786149     -0.099517       1.152911      1.309213\n",
      "13       0.666734      0.351127       0.414290      0.388929\n",
      "14      -1.005077      0.801772      -1.290219     -1.320168\n",
      "15      -1.005077      0.576450      -1.347036     -1.320168\n",
      "16      -0.049756      2.153707      -1.460669     -1.320168\n",
      "17      -0.288587     -1.226129       0.073389     -0.136947\n",
      "18       0.308489     -0.324840       0.527924      0.257460\n",
      "19       0.189074     -0.099517       0.584741      0.783336\n",
      "20      -0.527417      1.477740      -1.290219     -1.320168\n",
      "21       1.024980      0.125805       1.039277      1.572151\n",
      "22       0.905565     -0.324840       0.471107      0.125991\n",
      "23       0.308489     -1.000807       1.039277      0.257460\n",
      "24       0.666734     -0.550162       1.039277      1.309213\n",
      "25       1.024980     -0.099517       0.698375      0.651867\n",
      "26       0.905565     -0.099517       0.357473      0.257460\n",
      "27      -0.169171      1.703062      -1.176585     -1.188699\n",
      "28       0.786149     -0.099517       0.982460      0.783336\n",
      "29      -0.766247      0.801772      -1.347036     -1.320168, array([0, 2, 1, 0, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 2, 2, 0, 1, 0,\n",
      "       2, 0, 1, 2, 2, 0, 2, 0, 0, 1, 1, 0, 2, 2, 1, 1, 2, 1, 0, 1, 0, 2,\n",
      "       0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 2, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1,\n",
      "       2, 1, 1, 2, 0, 0, 0, 2, 1, 2, 1, 2, 2, 1, 0, 2, 1, 0, 2, 0, 2, 1,\n",
      "       1, 0, 1, 2, 0, 0, 2, 2, 2, 1, 2, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1,\n",
      "       0, 2, 1, 1, 0, 0, 0, 0, 1, 0]), array([0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2,\n",
      "       1, 2, 2, 1, 1, 0, 2, 0])] \n",
      "\n",
      "\n",
      "Before preprocessing:\n",
      "    CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0           1       1   19                  15                      39\n",
      "1           2       1   21                  15                      81\n",
      "2           3       0   20                  16                       6\n",
      "3           4       0   23                  16                      77\n",
      "4           5       0   31                  17                      40 \n",
      "\n",
      "worked\n",
      "After preprocessing:\n",
      "        Gender       Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0    1.141195 -1.425414           -1.779171               -0.435989\n",
      "1    1.141195 -1.282367           -1.779171                1.199413\n",
      "2   -0.876275 -1.353890           -1.739447               -1.720949\n",
      "3   -0.876275 -1.139319           -1.739447                1.043661\n",
      "4   -0.876275 -0.567131           -1.699723               -0.397051\n",
      "..        ...       ...                 ...                     ...\n",
      "193 -0.876275 -0.066466            2.113819                1.588795\n",
      "194 -0.876275  0.577246            2.391890               -1.331567\n",
      "195 -0.876275 -0.281037            2.391890                1.121537\n",
      "196 -0.876275  0.434198            2.630236               -0.864309\n",
      "197  1.141195 -0.495608            2.630236                0.926846\n",
      "\n",
      "[198 rows x 4 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Preprocessing\n",
    "\n",
    "for task_type, df in zip(task_type_list, df_list):\n",
    "    print(\"\\nBefore preprocessing:\\n\", df.head(), '\\n')\n",
    "    print(\"After preprocessing:\\n\", preprocessing(df, task_type), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data, task_type, label=None): #, alpha, threshold):\n",
    "    # If there's no label input\n",
    "    if label is None:\n",
    "        label = data.columns[-1]\n",
    "    \n",
    "    # NA\n",
    "    data.dropna()\n",
    "\n",
    "    # Remove duplicates\n",
    "    data.drop_duplicates()\n",
    "\n",
    "    # Handle outliers (IQR)\n",
    "    data = handling_outliers(data) #, alpha) # requires alpha value (default: 0.05)\n",
    "\n",
    "    # Handle ID-like column\n",
    "    data = handling_id_cols(data, label) #, threshold) # requires threshold (default: 0.99)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropna() False\n",
      "After dropna() False \n",
      "\n",
      "Before drop_duplicates() 1338\n",
      "After drop_duplicates() 1338 \n",
      "\n",
      "Before dropna() False\n",
      "After dropna() False \n",
      "\n",
      "Before drop_duplicates() 150\n",
      "After drop_duplicates() 150 \n",
      "\n",
      "Before dropna() False\n",
      "After dropna() False \n",
      "\n",
      "Before drop_duplicates() 200\n",
      "After drop_duplicates() 200 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test dropna and drop_duplicates\n",
    "for task_type in task_type_list:\n",
    "    \n",
    "    if task_type == 'regression':\n",
    "        print(\"Before dropna()\", df_reg.empty)\n",
    "        df_reg.dropna()\n",
    "        print(\"After dropna()\", df_reg.empty, '\\n')\n",
    "\n",
    "        print(\"Before drop_duplicates()\", len(df_reg.duplicated()))\n",
    "        df_reg.drop_duplicates()\n",
    "        print(\"After drop_duplicates()\", len(df_reg.duplicated()), '\\n')\n",
    "\n",
    "    elif task_type == 'classification':\n",
    "        print(\"Before dropna()\", df_clf.empty)\n",
    "        df_clf.dropna()\n",
    "        print(\"After dropna()\", df_clf.empty, '\\n')\n",
    "\n",
    "        print(\"Before drop_duplicates()\", len(df_clf.duplicated()))\n",
    "        df_clf.drop_duplicates()\n",
    "        print(\"After drop_duplicates()\", len(df_clf.duplicated()), '\\n')\n",
    "\n",
    "    elif task_type == 'clustering':\n",
    "        print(\"Before dropna()\", df_cls.empty)\n",
    "        df_cls.dropna()\n",
    "        print(\"After dropna()\", df_cls.empty, '\\n')\n",
    "\n",
    "        print(\"Before drop_duplicates()\", len(df_cls.duplicated()))\n",
    "        df_cls.drop_duplicates()\n",
    "        print(\"After drop_duplicates()\", len(df_cls.duplicated()), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before data_cleaning\n",
      "       age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns] \n",
      "\n",
      "Index(es) of outlier-contained records: {1031, 524, 1036, 14, 1037, 530, 19, 23, 1047, 1049, 29, 30, 543, 34, 549, 38, 39, 1062, 558, 1070, 49, 53, 1078, 55, 569, 1088, 577, 1090, 1096, 587, 82, 84, 86, 1111, 1117, 94, 1118, 609, 1122, 1124, 615, 109, 621, 623, 1139, 116, 629, 1146, 123, 1152, 1156, 146, 665, 667, 668, 158, 161, 674, 1186, 677, 682, 175, 689, 1206, 1207, 185, 697, 706, 1218, 203, 1230, 725, 1240, 1241, 223, 736, 1249, 738, 739, 742, 240, 242, 759, 251, 252, 254, 256, 1284, 263, 1288, 265, 1291, 271, 1300, 1301, 1303, 281, 286, 288, 1313, 803, 292, 1317, 298, 1323, 819, 312, 314, 826, 828, 322, 327, 328, 330, 842, 845, 847, 338, 850, 852, 856, 860, 883, 373, 377, 381, 893, 901, 401, 917, 420, 421, 422, 947, 951, 441, 953, 956, 958, 476, 488, 500, 1012, 1021, 1022} \n",
      "\n",
      "ID-like column: [] \n",
      "\n",
      "After data_cleaning\n",
      "       age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1193 rows x 7 columns] \n",
      "\n",
      "Before data_cleaning\n",
      "       Id  SepalLengthCm  ...  PetalWidthCm         Species\n",
      "0      1            5.1  ...           0.2     Iris-setosa\n",
      "1      2            4.9  ...           0.2     Iris-setosa\n",
      "2      3            4.7  ...           0.2     Iris-setosa\n",
      "3      4            4.6  ...           0.2     Iris-setosa\n",
      "4      5            5.0  ...           0.2     Iris-setosa\n",
      "..   ...            ...  ...           ...             ...\n",
      "145  146            6.7  ...           2.3  Iris-virginica\n",
      "146  147            6.3  ...           1.9  Iris-virginica\n",
      "147  148            6.5  ...           2.0  Iris-virginica\n",
      "148  149            6.2  ...           2.3  Iris-virginica\n",
      "149  150            5.9  ...           1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 6 columns] \n",
      "\n",
      "Index(es) of outlier-contained records: set() \n",
      "\n",
      "ID-like column: ['Id'] \n",
      "\n",
      "After data_cleaning\n",
      "      SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm         Species\n",
      "0              5.1           3.5            1.4           0.2     Iris-setosa\n",
      "1              4.9           3.0            1.4           0.2     Iris-setosa\n",
      "2              4.7           3.2            1.3           0.2     Iris-setosa\n",
      "3              4.6           3.1            1.5           0.2     Iris-setosa\n",
      "4              5.0           3.6            1.4           0.2     Iris-setosa\n",
      "..             ...           ...            ...           ...             ...\n",
      "145            6.7           3.0            5.2           2.3  Iris-virginica\n",
      "146            6.3           2.5            5.0           1.9  Iris-virginica\n",
      "147            6.5           3.0            5.2           2.0  Iris-virginica\n",
      "148            6.2           3.4            5.4           2.3  Iris-virginica\n",
      "149            5.9           3.0            5.1           1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns] \n",
      "\n",
      "Before data_cleaning\n",
      "      CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0             1    Male   19                  15                      39\n",
      "1             2    Male   21                  15                      81\n",
      "2             3  Female   20                  16                       6\n",
      "3             4  Female   23                  16                      77\n",
      "4             5  Female   31                  17                      40\n",
      "..          ...     ...  ...                 ...                     ...\n",
      "195         196  Female   35                 120                      79\n",
      "196         197  Female   45                 126                      28\n",
      "197         198    Male   32                 126                      74\n",
      "198         199    Male   32                 137                      18\n",
      "199         200    Male   30                 137                      83\n",
      "\n",
      "[200 rows x 5 columns] \n",
      "\n",
      "Index(es) of outlier-contained records: {198, 199} \n",
      "\n",
      "ID-like column: ['CustomerID'] \n",
      "\n",
      "After data_cleaning\n",
      "      Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0      Male   19                  15                      39\n",
      "1      Male   21                  15                      81\n",
      "2    Female   20                  16                       6\n",
      "3    Female   23                  16                      77\n",
      "4    Female   31                  17                      40\n",
      "..      ...  ...                 ...                     ...\n",
      "193  Female   38                 113                      91\n",
      "194  Female   47                 120                      16\n",
      "195  Female   35                 120                      79\n",
      "196  Female   45                 126                      28\n",
      "197    Male   32                 126                      74\n",
      "\n",
      "[198 rows x 4 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test data_cleaning\n",
    "for task_type, df in task_df_zip:\n",
    "    print(\"Before data_cleaning\\n\", df, '\\n')\n",
    "    cleaned_data = data_cleaning(df, task_type)\n",
    "    print(\"After data_cleaning\\n\", cleaned_data, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_outliers(df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Handles outliers by dropping the records containing outlier(s).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        alpha (float): The alpha value as the threshold to determine the normality of a data (default: 0.05).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with records containing outlier(s) removed.\n",
    "    \"\"\"\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns # Retrieving numeric features from the dataset\n",
    "    outlier_indices = set()  # Use a set to store unique indices of outlier rows\n",
    "\n",
    "    # Iterating through each numeric features\n",
    "    for numeric_feature in numeric_features:\n",
    "        data = df[numeric_feature]          # Store the selected column into an object called \"data\"\n",
    "\n",
    "        _, p = shapiro(data)                # Retrieving p velue evaluated from the Shapiro-Wilk Statistical test\n",
    "\n",
    "        if p > alpha:\n",
    "            pass                            # Skipping normally distributed data\n",
    "        else:\n",
    "            q1 = data.quantile(0.25)        # Retrieving the value of the 1st quantile (25%)\n",
    "            q3 = data.quantile(0.75)        # Retrieving the value of the 3rd quantile (75%)\n",
    "            iqr = q3 - q1                   # Interquartile range\n",
    "\n",
    "            # Define bounds for outliers\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            # Identify outlier indices\n",
    "            outliers = df[(data < lower_bound) | (data > upper_bound)].index\n",
    "            outlier_indices.update(outliers)  # Add these indices to the set\n",
    "    \n",
    "    # # Testing\n",
    "    # print(\"Index(es) of outlier-contained records:\", outlier_indices, '\\n')\n",
    "    \n",
    "    # Drop rows containing outliers\n",
    "    return df.drop(index=outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of outlier-contained records: {1031, 524, 1036, 14, 1037, 530, 19, 23, 1047, 1049, 29, 30, 543, 34, 549, 38, 39, 1062, 558, 1070, 49, 53, 1078, 55, 569, 1088, 577, 1090, 1096, 587, 82, 84, 86, 1111, 1117, 94, 1118, 609, 1122, 1124, 615, 109, 621, 623, 1139, 116, 629, 1146, 123, 1152, 1156, 146, 665, 667, 668, 158, 161, 674, 1186, 677, 682, 175, 689, 1206, 1207, 185, 697, 706, 1218, 203, 1230, 725, 1240, 1241, 223, 736, 1249, 738, 739, 742, 240, 242, 759, 251, 252, 254, 256, 1284, 263, 1288, 265, 1291, 271, 1300, 1301, 1303, 281, 286, 288, 1313, 803, 292, 1317, 298, 1323, 819, 312, 314, 826, 828, 322, 327, 328, 330, 842, 845, 847, 338, 850, 852, 856, 860, 883, 373, 377, 381, 893, 901, 401, 917, 420, 421, 422, 947, 951, 441, 953, 956, 958, 476, 488, 500, 1012, 1021, 1022} \n",
      "\n",
      "Index of outlier-contained records: set() \n",
      "\n",
      "Index of outlier-contained records: {198, 199} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test handling_outliers\n",
    "for task_type in task_type_list:\n",
    "    \n",
    "    if task_type == 'regression':\n",
    "        handling_outliers(df_reg)\n",
    "\n",
    "    elif task_type == 'classification':\n",
    "        handling_outliers(df_clf)  \n",
    "\n",
    "    elif task_type == 'clustering':\n",
    "        handling_outliers(df_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Handling ID-like Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_id_cols(df, label=None, threshold=0.99999):\n",
    "    \"\"\"\n",
    "    Handles id-like columns by dropping those with high cardinality.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        label (str): The label column to exclude from removal (default: None).\n",
    "        threshold (float): The cardinality threshold to identify id-like columns (default: 0.99).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with id-like columns removed.\n",
    "    \"\"\"\n",
    "    # If there's no label input\n",
    "    if label is None:\n",
    "        label = df.columns[-1]\n",
    "\n",
    "    # Identify id-like columns\n",
    "    id_like_cols = [\n",
    "        col for col in df.columns\n",
    "        if df[col].nunique() / len(df) > threshold and col != label\n",
    "    ]\n",
    "\n",
    "    # # Testing\n",
    "    # print(\"ID-like column:\", id_like_cols, '\\n')\n",
    "\n",
    "    # Drop id-like columns\n",
    "    return df.drop(columns=id_like_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID-like column: [] \n",
      "\n",
      "ID-like column: ['Id'] \n",
      "\n",
      "ID-like column: ['CustomerID'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test handling_id_cols\n",
    "for task_type in task_type_list:\n",
    "    \n",
    "    if task_type == 'regression':\n",
    "        handling_id_cols(df_reg)\n",
    "\n",
    "    elif task_type == 'classification':\n",
    "        handling_id_cols(df_clf)  \n",
    "\n",
    "    elif task_type == 'clustering':\n",
    "        handling_id_cols(df_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "def data_transformation(data, task_type, label=None):\n",
    "        \n",
    "    # For supervised task\n",
    "    if task_type in ('regression', 'classification'):\n",
    "\n",
    "        # If there's no label input\n",
    "        if label is None:\n",
    "            label = data.columns[-1]\n",
    "\n",
    "        # All column name\n",
    "        colnames = data.columns\n",
    "\n",
    "        # Retirieving categorical feature names\n",
    "        feature_names = data.drop(columns=label).select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "        # Feature-target split\n",
    "        X, y = feature_target_split(data, label)\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "        # Encoding (X_train, X_test, y_train, y_test)\n",
    "        data = feature_target_encoding(X_train, X_test, y_train, y_test, feature_names)\n",
    "\n",
    "        # Feature Scaling\n",
    "        data[0] = pd.DataFrame(self.scaler.fit_transform(data[0]), columns=colnames)    # Scaling X_train\n",
    "        data[1] = pd.DataFrame(self.scaler.transform(data[1]), columns=colnames)        # Scaling X_test\n",
    "\n",
    "    # For unsupervised task\n",
    "    elif task_type == 'clustering':\n",
    "\n",
    "        # All column name\n",
    "        colnames = data.columns\n",
    "\n",
    "        # Encoding\n",
    "        data = feature_encoding(data)\n",
    "\n",
    "        # Scaling\n",
    "        data = pd.DataFrame(self.scaler.fit_transform(data), columns=colnames)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev purps\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def data_transformation(data, task_type, label=None):\n",
    "\n",
    "    # For supervised task\n",
    "    if task_type in ('regression', 'classification'):\n",
    "\n",
    "        # If there's no label input\n",
    "        if label is None:\n",
    "            label = data.columns[-1]\n",
    "\n",
    "        # All column name\n",
    "        colnames = data.drop(columns=label).columns            \n",
    "\n",
    "        # Retirieving categorical feature names\n",
    "        feature_names = data.drop(columns=label).select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "        # Feature-target split\n",
    "        X, y = feature_target_split(data, label)\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if task_type == 'classification' else None)\n",
    "\n",
    "        # Encoding (X_train, X_test, y_train, y_test)\n",
    "        data = feature_target_encoding(X_train, X_test, y_train, y_test, task_type, feature_names)\n",
    "\n",
    "        # Feature Scaling\n",
    "        data[0] = pd.DataFrame(scaler.fit_transform(data[0]), columns=colnames)    # Scaling X_train\n",
    "        data[1] = pd.DataFrame(scaler.transform(data[1]), columns=colnames)        # Scaling X_test\n",
    "\n",
    "    # For unsupervised task\n",
    "    elif task_type == 'clustering':\n",
    "\n",
    "        # All column name\n",
    "        colnames = data.columns\n",
    "        \n",
    "        # Encoding\n",
    "        data = feature_encoding(data)\n",
    "\n",
    "        # Scaling\n",
    "        data = pd.DataFrame(scaler.fit_transform(data), columns=colnames)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Data Transformation:\n",
      "       age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns] \n",
      "\n",
      "After Data Transformation:\n",
      " [          age       sex       bmi  children    smoker    region\n",
      "0    0.347385  1.023333 -0.447469  0.766327 -0.342193  1.374625\n",
      "1    1.058097  1.023333  1.629846 -0.059748 -0.342193  1.374625\n",
      "2    1.768810 -0.977199  1.536196  1.592401 -0.342193  0.467087\n",
      "3    0.631670  1.023333  0.029292  0.766327 -0.342193  1.374625\n",
      "4   -0.079043  1.023333 -1.516776  1.592401 -0.342193  0.467087\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "949  1.200240 -0.977199  2.023174 -0.885822 -0.342193  0.467087\n",
      "950  0.205242  1.023333  1.217788  0.766327 -0.342193  0.467087\n",
      "951  1.129168 -0.977199  0.080373 -0.885822 -0.342193  1.374625\n",
      "952 -0.292256  1.023333 -0.498551 -0.059748 -0.342193  1.374625\n",
      "953  0.134171 -0.977199 -0.336793 -0.059748 -0.342193  0.467087\n",
      "\n",
      "[954 rows x 6 columns],           age       sex       bmi  children    smoker    region\n",
      "0   -0.150114  1.023333  2.809829  1.592401 -0.342193  0.467087\n",
      "1   -1.074040  1.023333  0.613324  2.418476 -0.342193 -1.347988\n",
      "2   -1.429396  1.023333 -0.226116 -0.885822 -0.342193  1.374625\n",
      "3   -0.860826 -0.977199  0.063346  1.592401 -0.342193 -0.440451\n",
      "4   -1.429396  1.023333  0.936840 -0.885822 -0.342193 -0.440451\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "234 -0.789755  1.023333 -1.279247  0.766327 -0.342193 -1.347988\n",
      "235 -1.500467  1.023333 -0.826324 -0.885822  2.922328 -1.347988\n",
      "236  0.631670  1.023333  0.128049  1.592401 -0.342193 -1.347988\n",
      "237 -0.150114 -0.977199 -0.617741 -0.885822  2.922328  0.467087\n",
      "238  1.413453  1.023333 -0.777796 -0.885822 -0.342193 -0.440451\n",
      "\n",
      "[239 rows x 6 columns], 74       7726.8540\n",
      "162     10450.5520\n",
      "603     16085.1275\n",
      "424      8968.3300\n",
      "408      6652.5288\n",
      "           ...    \n",
      "1172    11093.6229\n",
      "1227     7162.0122\n",
      "1266    10704.4700\n",
      "965      4746.3440\n",
      "1262     6770.1925\n",
      "Name: charges, Length: 954, dtype: float64, 660      6435.62370\n",
      "754     17128.42608\n",
      "487      1253.93600\n",
      "429     18804.75240\n",
      "559      1646.42970\n",
      "           ...     \n",
      "813      4428.88785\n",
      "157     15518.18025\n",
      "645     10141.13620\n",
      "750     19539.24300\n",
      "1015    12124.99240\n",
      "Name: charges, Length: 239, dtype: float64] \n",
      "\n",
      "Before Data Transformation:\n",
      "       Id  SepalLengthCm  ...  PetalWidthCm         Species\n",
      "0      1            5.1  ...           0.2     Iris-setosa\n",
      "1      2            4.9  ...           0.2     Iris-setosa\n",
      "2      3            4.7  ...           0.2     Iris-setosa\n",
      "3      4            4.6  ...           0.2     Iris-setosa\n",
      "4      5            5.0  ...           0.2     Iris-setosa\n",
      "..   ...            ...  ...           ...             ...\n",
      "145  146            6.7  ...           2.3  Iris-virginica\n",
      "146  147            6.3  ...           1.9  Iris-virginica\n",
      "147  148            6.5  ...           2.0  Iris-virginica\n",
      "148  149            6.2  ...           2.3  Iris-virginica\n",
      "149  150            5.9  ...           1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 6 columns] \n",
      "\n",
      "After Data Transformation:\n",
      " [     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "0        -1.721568     -0.324840      -1.347036     -1.320168\n",
      "1        -1.124492     -1.226129       0.414290      0.651867\n",
      "2         1.144395     -0.550162       0.584741      0.257460\n",
      "3        -1.124492      0.125805      -1.290219     -1.451638\n",
      "4        -0.408002     -1.226129       0.130206      0.125991\n",
      "..             ...           ...            ...           ...\n",
      "115      -1.124492      0.125805      -1.290219     -1.451638\n",
      "116      -1.363322      0.351127      -1.403853     -1.320168\n",
      "117      -0.408002      2.604352      -1.347036     -1.320168\n",
      "118       1.263810      0.125805       0.641558      0.388929\n",
      "119      -1.482738      0.125805      -1.290219     -1.320168\n",
      "\n",
      "[120 rows x 4 columns],     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "0       -1.721568     -0.099517      -1.403853     -1.320168\n",
      "1        0.308489     -0.099517       0.641558      0.783336\n",
      "2       -1.124492     -1.451452      -0.267513     -0.268416\n",
      "3       -1.005077     -1.676774      -0.267513     -0.268416\n",
      "4       -1.721568      0.351127      -1.403853     -1.320168\n",
      "5        0.547319      0.576450       0.527924      0.520398\n",
      "6       -1.482738      1.252417      -1.574303     -1.320168\n",
      "7       -0.527417      0.801772      -1.176585     -1.320168\n",
      "8        0.786149     -0.099517       0.812009      1.046275\n",
      "9       -0.527417     -0.099517       0.414290      0.388929\n",
      "10       1.741470     -0.324840       1.436996      0.783336\n",
      "11       1.263810      0.125805       0.755192      1.440682\n",
      "12       0.786149     -0.099517       1.152911      1.309213\n",
      "13       0.666734      0.351127       0.414290      0.388929\n",
      "14      -1.005077      0.801772      -1.290219     -1.320168\n",
      "15      -1.005077      0.576450      -1.347036     -1.320168\n",
      "16      -0.049756      2.153707      -1.460669     -1.320168\n",
      "17      -0.288587     -1.226129       0.073389     -0.136947\n",
      "18       0.308489     -0.324840       0.527924      0.257460\n",
      "19       0.189074     -0.099517       0.584741      0.783336\n",
      "20      -0.527417      1.477740      -1.290219     -1.320168\n",
      "21       1.024980      0.125805       1.039277      1.572151\n",
      "22       0.905565     -0.324840       0.471107      0.125991\n",
      "23       0.308489     -1.000807       1.039277      0.257460\n",
      "24       0.666734     -0.550162       1.039277      1.309213\n",
      "25       1.024980     -0.099517       0.698375      0.651867\n",
      "26       0.905565     -0.099517       0.357473      0.257460\n",
      "27      -0.169171      1.703062      -1.176585     -1.188699\n",
      "28       0.786149     -0.099517       0.982460      0.783336\n",
      "29      -0.766247      0.801772      -1.347036     -1.320168, array([0, 2, 1, 0, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 2, 2, 0, 1, 0,\n",
      "       2, 0, 1, 2, 2, 0, 2, 0, 0, 1, 1, 0, 2, 2, 1, 1, 2, 1, 0, 1, 0, 2,\n",
      "       0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 2, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1,\n",
      "       2, 1, 1, 2, 0, 0, 0, 2, 1, 2, 1, 2, 2, 1, 0, 2, 1, 0, 2, 0, 2, 1,\n",
      "       1, 0, 1, 2, 0, 0, 2, 2, 2, 1, 2, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1,\n",
      "       0, 2, 1, 1, 0, 0, 0, 0, 1, 0]), array([0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2,\n",
      "       1, 2, 2, 1, 1, 0, 2, 0])] \n",
      "\n",
      "Before Data Transformation:\n",
      "      CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0             1       1   19                  15                      39\n",
      "1             2       1   21                  15                      81\n",
      "2             3       0   20                  16                       6\n",
      "3             4       0   23                  16                      77\n",
      "4             5       0   31                  17                      40\n",
      "..          ...     ...  ...                 ...                     ...\n",
      "195         196       0   35                 120                      79\n",
      "196         197       0   45                 126                      28\n",
      "197         198       1   32                 126                      74\n",
      "198         199       1   32                 137                      18\n",
      "199         200       1   30                 137                      83\n",
      "\n",
      "[200 rows x 5 columns] \n",
      "\n",
      "worked\n",
      "After Data Transformation:\n",
      "        Gender       Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0    1.141195 -1.425414           -1.779171               -0.435989\n",
      "1    1.141195 -1.282367           -1.779171                1.199413\n",
      "2   -0.876275 -1.353890           -1.739447               -1.720949\n",
      "3   -0.876275 -1.139319           -1.739447                1.043661\n",
      "4   -0.876275 -0.567131           -1.699723               -0.397051\n",
      "..        ...       ...                 ...                     ...\n",
      "193 -0.876275 -0.066466            2.113819                1.588795\n",
      "194 -0.876275  0.577246            2.391890               -1.331567\n",
      "195 -0.876275 -0.281037            2.391890                1.121537\n",
      "196 -0.876275  0.434198            2.630236               -0.864309\n",
      "197  1.141195 -0.495608            2.630236                0.926846\n",
      "\n",
      "[198 rows x 4 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing data transformation\n",
    "for task_type, df in zip(task_type_list, df_list):\n",
    "    print(\"Before Data Transformation:\\n\", df, '\\n')\n",
    "\n",
    "    cleaned_df = data_cleaning(df, task_type)\n",
    "    transformed_df = data_transformation(cleaned_df, task_type)\n",
    "\n",
    "    print(\"After Data Transformation:\\n\", transformed_df, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Data Splitting (Feature-Target Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_target_split(data, label=None):\n",
    "    # If there's no label input\n",
    "    if label is None:\n",
    "        label = data.columns[-1]\n",
    "    \n",
    "    if label:\n",
    "        X = data.drop(columns=label)\n",
    "        y = data[label]\n",
    "    else:\n",
    "        X = data.iloc[:,:-1]\n",
    "        y = data.iloc[:,-1]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ft-split:\n",
      "       age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns] \n",
      "\n",
      "After ft-split:\n",
      "\n",
      "X:\n",
      "       age     sex     bmi  children smoker     region\n",
      "0      19  female  27.900         0    yes  southwest\n",
      "1      18    male  33.770         1     no  southeast\n",
      "2      28    male  33.000         3     no  southeast\n",
      "3      33    male  22.705         0     no  northwest\n",
      "4      32    male  28.880         0     no  northwest\n",
      "...   ...     ...     ...       ...    ...        ...\n",
      "1333   50    male  30.970         3     no  northwest\n",
      "1334   18  female  31.920         0     no  northeast\n",
      "1335   18  female  36.850         0     no  southeast\n",
      "1336   21  female  25.800         0     no  southwest\n",
      "1337   61  female  29.070         0    yes  northwest\n",
      "\n",
      "[1338 rows x 6 columns] \n",
      "\n",
      "y:\n",
      " 0       16884.92400\n",
      "1        1725.55230\n",
      "2        4449.46200\n",
      "3       21984.47061\n",
      "4        3866.85520\n",
      "           ...     \n",
      "1333    10600.54830\n",
      "1334     2205.98080\n",
      "1335     1629.83350\n",
      "1336     2007.94500\n",
      "1337    29141.36030\n",
      "Name: charges, Length: 1338, dtype: float64 \n",
      "\n",
      "Before ft-split:\n",
      "       Id  SepalLengthCm  ...  PetalWidthCm         Species\n",
      "0      1            5.1  ...           0.2     Iris-setosa\n",
      "1      2            4.9  ...           0.2     Iris-setosa\n",
      "2      3            4.7  ...           0.2     Iris-setosa\n",
      "3      4            4.6  ...           0.2     Iris-setosa\n",
      "4      5            5.0  ...           0.2     Iris-setosa\n",
      "..   ...            ...  ...           ...             ...\n",
      "145  146            6.7  ...           2.3  Iris-virginica\n",
      "146  147            6.3  ...           1.9  Iris-virginica\n",
      "147  148            6.5  ...           2.0  Iris-virginica\n",
      "148  149            6.2  ...           2.3  Iris-virginica\n",
      "149  150            5.9  ...           1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 6 columns] \n",
      "\n",
      "After ft-split:\n",
      "\n",
      "X:\n",
      "       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "0      1            5.1           3.5            1.4           0.2\n",
      "1      2            4.9           3.0            1.4           0.2\n",
      "2      3            4.7           3.2            1.3           0.2\n",
      "3      4            4.6           3.1            1.5           0.2\n",
      "4      5            5.0           3.6            1.4           0.2\n",
      "..   ...            ...           ...            ...           ...\n",
      "145  146            6.7           3.0            5.2           2.3\n",
      "146  147            6.3           2.5            5.0           1.9\n",
      "147  148            6.5           3.0            5.2           2.0\n",
      "148  149            6.2           3.4            5.4           2.3\n",
      "149  150            5.9           3.0            5.1           1.8\n",
      "\n",
      "[150 rows x 5 columns] \n",
      "\n",
      "y:\n",
      " 0         Iris-setosa\n",
      "1         Iris-setosa\n",
      "2         Iris-setosa\n",
      "3         Iris-setosa\n",
      "4         Iris-setosa\n",
      "            ...      \n",
      "145    Iris-virginica\n",
      "146    Iris-virginica\n",
      "147    Iris-virginica\n",
      "148    Iris-virginica\n",
      "149    Iris-virginica\n",
      "Name: Species, Length: 150, dtype: object \n",
      "\n",
      "Before ft-split:\n",
      "      CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0             1    Male   19                  15                      39\n",
      "1             2    Male   21                  15                      81\n",
      "2             3  Female   20                  16                       6\n",
      "3             4  Female   23                  16                      77\n",
      "4             5  Female   31                  17                      40\n",
      "..          ...     ...  ...                 ...                     ...\n",
      "195         196  Female   35                 120                      79\n",
      "196         197  Female   45                 126                      28\n",
      "197         198    Male   32                 126                      74\n",
      "198         199    Male   32                 137                      18\n",
      "199         200    Male   30                 137                      83\n",
      "\n",
      "[200 rows x 5 columns] \n",
      "\n",
      "After ft-split:\n",
      "\n",
      "X:\n",
      "      CustomerID  Gender  Age  Annual Income (k$)\n",
      "0             1    Male   19                  15\n",
      "1             2    Male   21                  15\n",
      "2             3  Female   20                  16\n",
      "3             4  Female   23                  16\n",
      "4             5  Female   31                  17\n",
      "..          ...     ...  ...                 ...\n",
      "195         196  Female   35                 120\n",
      "196         197  Female   45                 126\n",
      "197         198    Male   32                 126\n",
      "198         199    Male   32                 137\n",
      "199         200    Male   30                 137\n",
      "\n",
      "[200 rows x 4 columns] \n",
      "\n",
      "y:\n",
      " 0      39\n",
      "1      81\n",
      "2       6\n",
      "3      77\n",
      "4      40\n",
      "       ..\n",
      "195    79\n",
      "196    28\n",
      "197    74\n",
      "198    18\n",
      "199    83\n",
      "Name: Spending Score (1-100), Length: 200, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing feature_target_split\n",
    "for task_type, df in task_df_zip:\n",
    "    print(\"Before ft-split:\\n\", df, '\\n')\n",
    "    \n",
    "    X, y = feature_target_split(df)\n",
    "    \n",
    "    print(\"After ft-split:\\n\")\n",
    "    print(\"X:\\n\", X, '\\n')\n",
    "    print(\"y:\\n\", y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Categorical Feature/Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2.1 Feature-Target Encoding (Supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "def feature_target_encoding(X_train, X_test, y_train, y_test, task_type, feature_names=None):\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    # Instantiating encoders dicationary\n",
    "    feature_encoders = {}\n",
    "    target_encoder = None\n",
    "    \n",
    "    # Encoding each column through iteration\n",
    "    for feature in feature_names:\n",
    "\n",
    "        # Instantiate LabelEncoder object\n",
    "        fe = LabelEncoder()\n",
    "\n",
    "        # Fit and transform the features of the train set\n",
    "        X_train[feature] = fe.fit_transform(X_train[feature])\n",
    "\n",
    "        # Fit and transform the features of the test set\n",
    "        X_test[feature] = fe.transform(X_test[feature])\n",
    "\n",
    "        # Store the fitted feature encoders\n",
    "        feature_encoders[feature] = fe\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        # Instantiate the encoder object for target\n",
    "        te = LabelEncoder()\n",
    "\n",
    "        # Encoding the target of the train set\n",
    "        y_train = te.fit_transform(y_train)\n",
    "\n",
    "        # Encoding the target of the test set\n",
    "        y_test = te.transform(y_test)\n",
    "\n",
    "        # Store the fitted target encoder\n",
    "        target_encoder = te\n",
    "\n",
    "    # Store all the fitted encoders\n",
    "    self.encoders['feature_encoders'] = feature_encoders\n",
    "    self.encoders['target_encoder'] = target_encoder\n",
    "\n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev purps\n",
    "all_encoder = {}\n",
    "\n",
    "def feature_target_encoding(X_train, X_test, y_train, y_test, task_type, feature_names=None):\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    # Instantiating encoders dicationary\n",
    "    feature_encoders = {}\n",
    "    target_encoder = None\n",
    "    \n",
    "    # Encoding each column through iteration\n",
    "    for feature in feature_names:\n",
    "\n",
    "        # Instantiate LabelEncoder object\n",
    "        fe = LabelEncoder()\n",
    "\n",
    "        # Fit and transform the features of the train set\n",
    "        X_train[feature] = fe.fit_transform(X_train[feature])\n",
    "\n",
    "        # Fit and transform the features of the test set\n",
    "        X_test[feature] = fe.transform(X_test[feature])\n",
    "\n",
    "        # Store the fitted feature encoders\n",
    "        feature_encoders[feature] = fe\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        # Instantiate the encoder object for target\n",
    "        te = LabelEncoder()\n",
    "\n",
    "        # Encoding the target of the train set\n",
    "        y_train = te.fit_transform(y_train)\n",
    "\n",
    "        # Encoding the target of the test set\n",
    "        y_test = te.transform(y_test)\n",
    "\n",
    "        # Store the fitted target encoder\n",
    "        target_encoder = te\n",
    "\n",
    "    # print(target_encoder)\n",
    "\n",
    "    # Store all the fitted encoders\n",
    "    all_encoder['feature_encoders'] = feature_encoders\n",
    "    all_encoder['target_encoder'] = target_encoder\n",
    "\n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before feature-target encoding:\n",
      "       age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns] \n",
      "\n",
      "After encoding:\n",
      " [      age  sex     bmi  children  smoker  region\n",
      "560    46    0  19.950         2       0       1\n",
      "1285   47    0  24.320         0       0       0\n",
      "1142   52    0  24.860         0       0       2\n",
      "969    39    0  34.320         5       0       2\n",
      "486    54    0  21.470         3       0       1\n",
      "...   ...  ...     ...       ...     ...     ...\n",
      "1095   18    0  31.350         4       0       0\n",
      "1130   39    0  23.870         5       0       2\n",
      "1294   58    1  25.175         0       0       0\n",
      "860    37    0  47.600         2       1       3\n",
      "1126   55    1  29.900         0       0       3\n",
      "\n",
      "[1070 rows x 6 columns],       age  sex     bmi  children  smoker  region\n",
      "764    45    0  25.175         2       0       0\n",
      "887    36    0  30.020         0       0       1\n",
      "890    64    0  26.885         0       1       1\n",
      "1293   46    1  25.745         3       0       1\n",
      "259    19    1  31.920         0       1       1\n",
      "...   ...  ...     ...       ...     ...     ...\n",
      "109    63    1  35.090         0       1       2\n",
      "575    58    0  27.170         0       0       1\n",
      "535    38    1  28.025         1       0       0\n",
      "543    54    0  47.410         0       1       2\n",
      "846    51    0  34.200         1       0       3\n",
      "\n",
      "[268 rows x 6 columns], 560      9193.83850\n",
      "1285     8534.67180\n",
      "1142    27117.99378\n",
      "969      8596.82780\n",
      "486     12475.35130\n",
      "           ...     \n",
      "1095     4561.18850\n",
      "1130     8582.30230\n",
      "1294    11931.12525\n",
      "860     46113.51100\n",
      "1126    10214.63600\n",
      "Name: charges, Length: 1070, dtype: float64, 764      9095.06825\n",
      "887      5272.17580\n",
      "890     29330.98315\n",
      "1293     9301.89355\n",
      "259     33750.29180\n",
      "           ...     \n",
      "109     47055.53210\n",
      "575     12222.89830\n",
      "535      6067.12675\n",
      "543     63770.42801\n",
      "846      9872.70100\n",
      "Name: charges, Length: 268, dtype: float64] \n",
      "\n",
      "Before feature-target encoding:\n",
      "       Id  SepalLengthCm  ...  PetalWidthCm         Species\n",
      "0      1            5.1  ...           0.2     Iris-setosa\n",
      "1      2            4.9  ...           0.2     Iris-setosa\n",
      "2      3            4.7  ...           0.2     Iris-setosa\n",
      "3      4            4.6  ...           0.2     Iris-setosa\n",
      "4      5            5.0  ...           0.2     Iris-setosa\n",
      "..   ...            ...  ...           ...             ...\n",
      "145  146            6.7  ...           2.3  Iris-virginica\n",
      "146  147            6.3  ...           1.9  Iris-virginica\n",
      "147  148            6.5  ...           2.0  Iris-virginica\n",
      "148  149            6.2  ...           2.3  Iris-virginica\n",
      "149  150            5.9  ...           1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 6 columns] \n",
      "\n",
      "After encoding:\n",
      " [      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "22    23            4.6           3.6            1.0           0.2\n",
      "15    16            5.7           4.4            1.5           0.4\n",
      "65    66            6.7           3.1            4.4           1.4\n",
      "11    12            4.8           3.4            1.6           0.2\n",
      "42    43            4.4           3.2            1.3           0.2\n",
      "..   ...            ...           ...            ...           ...\n",
      "71    72            6.1           2.8            4.0           1.3\n",
      "106  107            4.9           2.5            4.5           1.7\n",
      "14    15            5.8           4.0            1.2           0.2\n",
      "92    93            5.8           2.6            4.0           1.2\n",
      "102  103            7.1           3.0            5.9           2.1\n",
      "\n",
      "[120 rows x 5 columns],       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "73    74            6.1           2.8            4.7           1.2\n",
      "18    19            5.7           3.8            1.7           0.3\n",
      "118  119            7.7           2.6            6.9           2.3\n",
      "78    79            6.0           2.9            4.5           1.5\n",
      "76    77            6.8           2.8            4.8           1.4\n",
      "31    32            5.4           3.4            1.5           0.4\n",
      "64    65            5.6           2.9            3.6           1.3\n",
      "141  142            6.9           3.1            5.1           2.3\n",
      "68    69            6.2           2.2            4.5           1.5\n",
      "82    83            5.8           2.7            3.9           1.2\n",
      "110  111            6.5           3.2            5.1           2.0\n",
      "12    13            4.8           3.0            1.4           0.1\n",
      "36    37            5.5           3.5            1.3           0.2\n",
      "9     10            4.9           3.1            1.5           0.1\n",
      "19    20            5.1           3.8            1.5           0.3\n",
      "56    57            6.3           3.3            4.7           1.6\n",
      "104  105            6.5           3.0            5.8           2.2\n",
      "69    70            5.6           2.5            3.9           1.1\n",
      "55    56            5.7           2.8            4.5           1.3\n",
      "132  133            6.4           2.8            5.6           2.2\n",
      "29    30            4.7           3.2            1.6           0.2\n",
      "127  128            6.1           3.0            4.9           1.8\n",
      "26    27            5.0           3.4            1.6           0.4\n",
      "128  129            6.4           2.8            5.6           2.1\n",
      "131  132            7.9           3.8            6.4           2.0\n",
      "145  146            6.7           3.0            5.2           2.3\n",
      "108  109            6.7           2.5            5.8           1.8\n",
      "143  144            6.8           3.2            5.9           2.3\n",
      "45    46            4.8           3.0            1.4           0.3\n",
      "30    31            4.8           3.1            1.6           0.2, array([0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 2, 2, 1, 2, 1, 2,\n",
      "       1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2,\n",
      "       1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 1, 2, 2, 1,\n",
      "       0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 1, 2, 1, 2, 0, 2, 1, 2,\n",
      "       1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 2, 2, 1, 2,\n",
      "       1, 1, 2, 2, 0, 1, 2, 0, 1, 2]), array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
      "       0, 2, 2, 2, 2, 2, 0, 0])] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing feature-target encoding\n",
    "for task_type, df in zip(task_type_list, df_list):\n",
    "    if task_type != 'clustering':\n",
    "        print(\"Before feature-target encoding:\\n\", df, '\\n')\n",
    "\n",
    "        X = df.iloc[:,:-1]\n",
    "        y = df.iloc[:,-1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        encoded_df = feature_target_encoding(X_train, X_test, y_train, y_test, task_type)\n",
    "\n",
    "        print(\"After encoding:\\n\", encoded_df, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2.2 Feature Encoding (Unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "def feature_encoding(data, feature_names=None):\n",
    "    if not feature_names:\n",
    "        feature_names = data.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    # Instantiating encoders dicationary\n",
    "    feature_encoders = {}\n",
    "\n",
    "    # Encoding each column through iteration\n",
    "    for feature in feature_names:\n",
    "\n",
    "        # Instantiate LabelEncoder object\n",
    "        fe = LabelEncoder()\n",
    "\n",
    "        # Fit and transform the features of the train set\n",
    "        data[feature] = fe.fit_transform(data[feature])\n",
    "\n",
    "        # Store the fitted feature encoders\n",
    "        feature_encoders[feature] = fe\n",
    "    \n",
    "    # Store all the fitted encoders\n",
    "    self.encoders['feature_encoders'] = feature_encoders\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev\n",
    "all_encoder = {}\n",
    "\n",
    "def feature_encoding(data, feature_names=None):\n",
    "    if not feature_names:\n",
    "        feature_names = data.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    # Instantiating encoders dicationary\n",
    "    feature_encoders = {}\n",
    "\n",
    "    # Encoding each column through iteration\n",
    "    for feature in feature_names:\n",
    "\n",
    "        # Instantiate LabelEncoder object\n",
    "        fe = LabelEncoder()\n",
    "\n",
    "        # Fit and transform the features of the train set\n",
    "        data[feature] = fe.fit_transform(data[feature])\n",
    "\n",
    "        # Store the fitted feature encoders\n",
    "        feature_encoders[feature] = fe\n",
    "    \n",
    "    # Store all the fitted encoders\n",
    "    all_encoder['feature_encoders'] = feature_encoders\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before feature encoding:\n",
      "      CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0             1    Male   19                  15                      39\n",
      "1             2    Male   21                  15                      81\n",
      "2             3  Female   20                  16                       6\n",
      "3             4  Female   23                  16                      77\n",
      "4             5  Female   31                  17                      40\n",
      "..          ...     ...  ...                 ...                     ...\n",
      "195         196  Female   35                 120                      79\n",
      "196         197  Female   45                 126                      28\n",
      "197         198    Male   32                 126                      74\n",
      "198         199    Male   32                 137                      18\n",
      "199         200    Male   30                 137                      83\n",
      "\n",
      "[200 rows x 5 columns] \n",
      "\n",
      "worked\n",
      "After encoding:\n",
      "      CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0             1       1   19                  15                      39\n",
      "1             2       1   21                  15                      81\n",
      "2             3       0   20                  16                       6\n",
      "3             4       0   23                  16                      77\n",
      "4             5       0   31                  17                      40\n",
      "..          ...     ...  ...                 ...                     ...\n",
      "195         196       0   35                 120                      79\n",
      "196         197       0   45                 126                      28\n",
      "197         198       1   32                 126                      74\n",
      "198         199       1   32                 137                      18\n",
      "199         200       1   30                 137                      83\n",
      "\n",
      "[200 rows x 5 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing feature encoding\n",
    "for task_type, df in task_df_zip:\n",
    "    if task_type == 'clustering':\n",
    "        print(\"Before feature encoding:\\n\", df, '\\n')\n",
    "\n",
    "        encoded_df = feature_encoding(df)\n",
    "\n",
    "        print(\"After encoding:\\n\", encoded_df, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV. Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **V. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed data\n",
    "\n",
    "preprocessed_df_reg = preprocessing(df_reg, task_type='regression')\n",
    "preprocessed_df_clf = preprocessing(df_clf, task_type='classification')\n",
    "preprocessed_df_cls = preprocessing(df_cls, task_type='clustering')\n",
    "\n",
    "preprocessed_df_list = [\n",
    "    preprocessed_df_reg,\n",
    "    preprocessed_df_clf,\n",
    "    preprocessed_df_cls,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def evaluate(data, task_type):\n",
    "\n",
    "    # Assign data into specified cases\n",
    "    data_dict = create_data_dict(data, task_type)\n",
    "\n",
    "    # Model and necessary variable(s)\n",
    "    model, reference_metric, n_obsv, n_predictors, n_classes, is_multioutput = model_and_variables(data_dict, task_type)\n",
    "\n",
    "    # For evaluation\n",
    "    metrics_names, eval_metrics = evaluation_metrics(task_type)\n",
    "\n",
    "    # Setting the min-max value\n",
    "    minmax_val = \"min\" if reference_metric in [\"Mean Squared Error (MSE)\",\n",
    "                                                \"Root Mean Squared Error (RMSE)\",\n",
    "                                                \"Mean Absolute Error (MAE)\",\n",
    "                                                \"Mean Absolute Percentage Error (MAPE)\",\n",
    "                                                \"Davies-Bouldin Index\",\n",
    "                                                ] else \"max\"\n",
    "\n",
    "    # Getting hyperparameter bounds and names\n",
    "    paras_bounds, paras_bounds_names = hyperparameters_bounds(model, random_state=42)\n",
    "\n",
    "    epoch = 10\n",
    "    pop_size = 10\n",
    "\n",
    "    # Assigning Metaheursitic Optimizer\n",
    "    optimizers = [\n",
    "        ACOR.OriginalACOR(epoch=epoch, pop_size=pop_size, sample_count = 25, intent_factor = 0.5, zeta = 1.0),\n",
    "        GA.BaseGA(epoch=epoch, pop_size=pop_size, pc=0.9, pm=0.05, selection=\"tournament\", k_way=0.4, crossover=\"multi_points\", mutation=\"swap\"), # Epoch & pop_size minimal 10\n",
    "        PSO.OriginalPSO(epoch=epoch, pop_size=pop_size, c1=2.05, c2=2.05, w=0.4),\n",
    "        SA.OriginalSA(epoch=epoch, pop_size=pop_size, temp_init=100, step_size=0.1),\n",
    "    ]\n",
    "\n",
    "    # List for containing evaluation values\n",
    "    metaopt_name = []\n",
    "    metaopt_object = []\n",
    "    ml_models = []\n",
    "    best_metrics = []\n",
    "    time_taken = []\n",
    "\n",
    "    # Evaluation through iteration\n",
    "    for optimizer in optimizers:\n",
    "\n",
    "        #  Defining the problem class\n",
    "        problem = OptimizedProblem(bounds=paras_bounds,\n",
    "                                    minmax=minmax_val,\n",
    "                                    data=data_dict,\n",
    "                                    optimizer=optimizer,\n",
    "                                    model=model,\n",
    "                                    task_type=task_type,\n",
    "                                    eval_metrics=eval_metrics,\n",
    "                                    paras_bounds_names=paras_bounds_names,\n",
    "                                    n_classes=n_classes,\n",
    "                                    is_multioutput=is_multioutput,\n",
    "                                    n_obsv=n_obsv,\n",
    "                                    n_predictors=n_predictors,\n",
    "                                    )\n",
    "\n",
    "        # Time monitoring and optimization process\n",
    "        start = time.perf_counter()\n",
    "        optimizer.solve(problem)\n",
    "        end = time.perf_counter() - start\n",
    "\n",
    "        metaopt_name.append(optimizer.__class__.__name__)\n",
    "        metaopt_object.append(optimizer)\n",
    "        ml_models.append(problem.best_model)\n",
    "        best_metrics.append(problem.best_metrics)\n",
    "        time_taken.append(end)\n",
    "\n",
    "        # print(f\"Best agent: {optimizer.g_best}\")\n",
    "        # print(f\"Best solution: {optimizer.g_best.solution}\")\n",
    "        # print(f\"Best {reference_metric}: {optimizer.g_best.target.fitness}\")\n",
    "        # print(f\"Best parameters: {optimizer.problem.decode_solution(optimizer.g_best.solution)}\")        \n",
    "\n",
    "    # Final result\n",
    "    result_df = pd.DataFrame ({\n",
    "        \"Metaheuristic Optimizer (Name)\" : metaopt_name,\n",
    "        \"Metaheuristic Optimizer (Object)\" : metaopt_object,\n",
    "        \"Machine Learning Model (object)\" : ml_models,\n",
    "        **{metric: values for metric, values in zip(metrics_names, zip(*best_metrics))},\n",
    "        \"Time taken (s)\" : time_taken,\n",
    "    })\n",
    "\n",
    "    # Save the trained model\n",
    "    best_ml_model = result_df.sort_values(by=reference_metric, ascending=False).iloc[0,2]\n",
    "    joblib.dump(best_ml_model, f'Best_{best_ml_model.__class__.__name__}.pkl')\n",
    "\n",
    "    return result_df, best_ml_model, data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/30 11:56:08 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: Solving single objective optimization problem.\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:56:11 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 1, Current best: 38740454.554215446, Global best: 38740454.554215446, Runtime: 2.19302 seconds\n",
      "2024/11/30 11:56:14 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 2, Current best: 19210362.179775853, Global best: 19210362.179775853, Runtime: 3.15476 seconds\n",
      "2024/11/30 11:56:16 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 3, Current best: 19210362.179775853, Global best: 19210362.179775853, Runtime: 2.58052 seconds\n",
      "2024/11/30 11:56:20 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 4, Current best: 19210362.179775853, Global best: 19210362.179775853, Runtime: 3.92898 seconds\n",
      "2024/11/30 11:56:28 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 5, Current best: 18046757.50034617, Global best: 18046757.50034617, Runtime: 7.43908 seconds\n",
      "2024/11/30 11:56:35 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 6, Current best: 18027728.79060437, Global best: 18027728.79060437, Runtime: 7.13881 seconds\n",
      "2024/11/30 11:56:42 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 7, Current best: 18027728.79060437, Global best: 18027728.79060437, Runtime: 7.28932 seconds\n",
      "2024/11/30 11:56:49 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 8, Current best: 17110969.809137125, Global best: 17110969.809137125, Runtime: 7.23093 seconds\n",
      "2024/11/30 11:57:01 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 9, Current best: 17110969.809137125, Global best: 17110969.809137125, Runtime: 11.25998 seconds\n",
      "2024/11/30 11:57:08 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 10, Current best: 17110969.809137125, Global best: 17110969.809137125, Runtime: 7.83354 seconds\n",
      "2024/11/30 11:57:09 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: Solving single objective optimization problem.\n",
      "2024/11/30 11:57:10 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 1, Current best: 39022854.32702425, Global best: 39022854.32702425, Runtime: 0.74924 seconds\n",
      "2024/11/30 11:57:11 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 2, Current best: 39032274.38424684, Global best: 39022854.32702425, Runtime: 0.60977 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:12 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 3, Current best: 38701393.03135314, Global best: 38701393.03135314, Runtime: 0.78293 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:13 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 4, Current best: 36459071.676011786, Global best: 36459071.676011786, Runtime: 1.08052 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:13 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 5, Current best: 36459071.676011786, Global best: 36459071.676011786, Runtime: 0.49659 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:14 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 6, Current best: 37272592.77410506, Global best: 36459071.676011786, Runtime: 0.69027 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:15 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 7, Current best: 35987223.83163916, Global best: 35987223.83163916, Runtime: 0.93107 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:16 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 8, Current best: 35987223.83163916, Global best: 35987223.83163916, Runtime: 0.72940 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:16 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 9, Current best: 35987223.83163916, Global best: 35987223.83163916, Runtime: 0.49335 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:17 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 10, Current best: 35987223.83163916, Global best: 35987223.83163916, Runtime: 0.78654 seconds\n",
      "2024/11/30 11:57:17 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: Solving single objective optimization problem.\n",
      "2024/11/30 11:57:19 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 1, Current best: 30066171.0415426, Global best: 30066171.0415426, Runtime: 0.73970 seconds\n",
      "2024/11/30 11:57:20 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 2, Current best: 18378810.806642834, Global best: 18378810.806642834, Runtime: 0.85485 seconds\n",
      "2024/11/30 11:57:20 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 3, Current best: 18378810.806642834, Global best: 18378810.806642834, Runtime: 0.54442 seconds\n",
      "2024/11/30 11:57:21 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 4, Current best: 18378810.806642834, Global best: 18378810.806642834, Runtime: 1.17468 seconds\n",
      "2024/11/30 11:57:23 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 5, Current best: 17973342.4676898, Global best: 17973342.4676898, Runtime: 2.15493 seconds\n",
      "2024/11/30 11:57:25 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 6, Current best: 17973205.5164343, Global best: 17973205.5164343, Runtime: 1.52636 seconds\n",
      "2024/11/30 11:57:27 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 7, Current best: 17831221.36336322, Global best: 17831221.36336322, Runtime: 1.63068 seconds\n",
      "2024/11/30 11:57:28 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 8, Current best: 17618522.260139205, Global best: 17618522.260139205, Runtime: 1.48495 seconds\n",
      "2024/11/30 11:57:29 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 9, Current best: 17618522.260139205, Global best: 17618522.260139205, Runtime: 0.89443 seconds\n",
      "2024/11/30 11:57:30 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 10, Current best: 17618522.260139205, Global best: 17618522.260139205, Runtime: 1.18732 seconds\n",
      "2024/11/30 11:57:30 PM, INFO, mealpy.physics_based.SA.OriginalSA: Solving single objective optimization problem.\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 1, Current best: 39148428.91315722, Global best: 39148428.91315722, Runtime: 0.03244 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 2, Current best: 38826674.28071463, Global best: 38826674.28071463, Runtime: 0.03615 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 3, Current best: 38826674.28071463, Global best: 38826674.28071463, Runtime: 0.02977 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 4, Current best: 38826674.28071463, Global best: 38826674.28071463, Runtime: 0.03634 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 5, Current best: 38826674.28071463, Global best: 38826674.28071463, Runtime: 0.03575 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 6, Current best: 38826674.28071463, Global best: 38826674.28071463, Runtime: 0.03975 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 7, Current best: 38826674.28071463, Global best: 38826674.28071463, Runtime: 0.03852 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 8, Current best: 38826674.28071463, Global best: 38826674.28071463, Runtime: 0.03586 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 9, Current best: 38826674.28071463, Global best: 38826674.28071463, Runtime: 0.03385 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 10, Current best: 38826674.28071463, Global best: 38826674.28071463, Runtime: 0.03792 seconds\n",
      "2024/11/30 11:57:31 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: Solving single objective optimization problem.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result df:\n",
      "  Metaheuristic Optimizer (Name)  ... Time taken (s)\n",
      "0                   OriginalACOR  ...      60.938422\n",
      "1                         BaseGA  ...       8.276748\n",
      "2                    OriginalPSO  ...      13.190174\n",
      "3                     OriginalSA  ...       1.031757\n",
      "\n",
      "[4 rows x 11 columns]\n",
      "\n",
      "Best ml model:\n",
      "RandomForestRegressor(ccp_alpha=38.254340300566895, criterion='poisson',\n",
      "                      max_depth=10, max_features=100, max_leaf_nodes=5,\n",
      "                      min_impurity_decrease=53.90340421721284,\n",
      "                      min_samples_leaf=77, min_samples_split=14,\n",
      "                      min_weight_fraction_leaf=0.418591320563665,\n",
      "                      n_estimators=40)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/30 11:57:35 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 1, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 2.55944 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:37 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 2, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.85532 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:40 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 3, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 3.24350 seconds\n",
      "2024/11/30 11:57:43 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 4, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 2.60897 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:45 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 5, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 2.22197 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:47 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 6, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 2.00825 seconds\n",
      "2024/11/30 11:57:49 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 7, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 2.25695 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:52 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 8, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 2.56763 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:54 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 9, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 2.16991 seconds\n",
      "2024/11/30 11:57:56 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 10, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 2.53805 seconds\n",
      "2024/11/30 11:57:57 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: Solving single objective optimization problem.\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:58 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 1, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.33762 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:58 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 2, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.50908 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:59 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 3, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.29987 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:57:59 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 4, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.59205 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:58:00 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 5, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.43191 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:58:00 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 6, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.40036 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:58:01 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 7, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.33588 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:58:01 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 8, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.44898 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:58:01 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 9, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.50600 seconds\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "2024/11/30 11:58:02 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 10, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.33063 seconds\n",
      "2024/11/30 11:58:02 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: Solving single objective optimization problem.\n",
      "2024/11/30 11:58:04 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 1, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.55165 seconds\n",
      "2024/11/30 11:58:05 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 2, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.03412 seconds\n",
      "2024/11/30 11:58:06 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 3, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.09491 seconds\n",
      "2024/11/30 11:58:07 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 4, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.16755 seconds\n",
      "2024/11/30 11:58:09 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 5, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.34051 seconds\n",
      "2024/11/30 11:58:10 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 6, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.36189 seconds\n",
      "2024/11/30 11:58:12 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 7, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.51481 seconds\n",
      "2024/11/30 11:58:13 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 8, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.38153 seconds\n",
      "2024/11/30 11:58:14 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 9, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.27511 seconds\n",
      "2024/11/30 11:58:16 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 10, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 1.26211 seconds\n",
      "2024/11/30 11:58:16 PM, INFO, mealpy.physics_based.SA.OriginalSA: Solving single objective optimization problem.\n",
      "2024/11/30 11:58:17 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 1, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.14691 seconds\n",
      "2024/11/30 11:58:17 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 2, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.22524 seconds\n",
      "2024/11/30 11:58:17 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 3, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.17550 seconds\n",
      "2024/11/30 11:58:17 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 4, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.17917 seconds\n",
      "2024/11/30 11:58:17 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 5, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.17602 seconds\n",
      "2024/11/30 11:58:18 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 6, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.17953 seconds\n",
      "2024/11/30 11:58:18 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 7, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.19147 seconds\n",
      "2024/11/30 11:58:18 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 8, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.17773 seconds\n",
      "2024/11/30 11:58:18 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 9, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.17670 seconds\n",
      "2024/11/30 11:58:18 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 10, Current best: 0.16666666666666666, Global best: 0.16666666666666666, Runtime: 0.17105 seconds\n",
      "2024/11/30 11:58:18 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: Solving single objective optimization problem.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result df:\n",
      "  Metaheuristic Optimizer (Name)  ... Time taken (s)\n",
      "0                   OriginalACOR  ...      25.048588\n",
      "1                         BaseGA  ...       5.153948\n",
      "2                    OriginalPSO  ...      13.709722\n",
      "3                     OriginalSA  ...       2.625116\n",
      "\n",
      "[4 rows x 8 columns]\n",
      "\n",
      "Best ml model:\n",
      "RandomForestClassifier(ccp_alpha=53.0334423681866, class_weight='balanced',\n",
      "                       max_features=10, max_leaf_nodes=33,\n",
      "                       min_impurity_decrease=100.0, min_samples_leaf=100,\n",
      "                       min_samples_split=25,\n",
      "                       min_weight_fraction_leaf=0.1341841304904617,\n",
      "                       n_estimators=96, n_jobs=10, oob_score=True,\n",
      "                       random_state=42)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/30 11:58:21 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 1, Current best: 0.6560343622437388, Global best: 0.6560343622437388, Runtime: 1.56753 seconds\n",
      "2024/11/30 11:58:23 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 2, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 2.01742 seconds\n",
      "2024/11/30 11:58:25 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 3, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 2.01038 seconds\n",
      "2024/11/30 11:58:26 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 4, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 1.88001 seconds\n",
      "2024/11/30 11:58:29 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 5, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 2.41712 seconds\n",
      "2024/11/30 11:58:32 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 6, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 2.65507 seconds\n",
      "2024/11/30 11:58:34 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 7, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 2.00214 seconds\n",
      "2024/11/30 11:58:35 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 8, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 1.97285 seconds\n",
      "2024/11/30 11:58:38 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 9, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 2.56094 seconds\n",
      "2024/11/30 11:58:41 PM, INFO, mealpy.swarm_based.ACOR.OriginalACOR: >>>Problem: P, Epoch: 10, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 2.61927 seconds\n",
      "2024/11/30 11:58:41 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: Solving single objective optimization problem.\n",
      "2024/11/30 11:58:42 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 1, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.42899 seconds\n",
      "2024/11/30 11:58:43 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 2, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.66352 seconds\n",
      "2024/11/30 11:58:43 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 3, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.45298 seconds\n",
      "2024/11/30 11:58:44 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 4, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.52892 seconds\n",
      "2024/11/30 11:58:44 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 5, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.25188 seconds\n",
      "2024/11/30 11:58:45 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 6, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.33572 seconds\n",
      "2024/11/30 11:58:45 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 7, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.42297 seconds\n",
      "2024/11/30 11:58:46 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 8, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.84389 seconds\n",
      "2024/11/30 11:58:46 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 9, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.42899 seconds\n",
      "2024/11/30 11:58:47 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 10, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.82990 seconds\n",
      "2024/11/30 11:58:47 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: Solving single objective optimization problem.\n",
      "2024/11/30 11:58:48 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 1, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.49128 seconds\n",
      "2024/11/30 11:58:49 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 2, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.49708 seconds\n",
      "2024/11/30 11:58:49 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 3, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.40743 seconds\n",
      "2024/11/30 11:58:50 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 4, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.26912 seconds\n",
      "2024/11/30 11:58:50 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 5, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.30268 seconds\n",
      "2024/11/30 11:58:50 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 6, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.30900 seconds\n",
      "2024/11/30 11:58:51 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 7, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.38836 seconds\n",
      "2024/11/30 11:58:51 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 8, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.45679 seconds\n",
      "2024/11/30 11:58:51 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 9, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.44160 seconds\n",
      "2024/11/30 11:58:52 PM, INFO, mealpy.swarm_based.PSO.OriginalPSO: >>>Problem: P, Epoch: 10, Current best: 0.6570362328827836, Global best: 0.6570362328827836, Runtime: 0.46564 seconds\n",
      "2024/11/30 11:58:52 PM, INFO, mealpy.physics_based.SA.OriginalSA: Solving single objective optimization problem.\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 1, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.06878 seconds\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 2, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.06486 seconds\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 3, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.06570 seconds\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 4, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.07478 seconds\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 5, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.07138 seconds\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 6, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.05378 seconds\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 7, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.04032 seconds\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 8, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.04411 seconds\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 9, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.04179 seconds\n",
      "2024/11/30 11:58:53 PM, INFO, mealpy.physics_based.SA.OriginalSA: >>>Problem: P, Epoch: 10, Current best: 0.6398054782007414, Global best: 0.6398054782007414, Runtime: 0.04466 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result df:\n",
      "  Metaheuristic Optimizer (Name)  ... Time taken (s)\n",
      "0                   OriginalACOR  ...      22.296300\n",
      "1                         BaseGA  ...       6.366074\n",
      "2                    OriginalPSO  ...       4.541927\n",
      "3                     OriginalSA  ...       1.092453\n",
      "\n",
      "[4 rows x 7 columns]\n",
      "\n",
      "Best ml model:\n",
      "KMeans(algorithm='elkan', max_iter=100, n_clusters=14, n_init=5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing evaluation block\n",
    "\n",
    "for task_type, preprocessed_df in zip(task_type_list, preprocessed_df_list):\n",
    "\n",
    "    result_df, best_ml_model, data_dict = evaluate(data=preprocessed_df, task_type=task_type)\n",
    "\n",
    "    print(f\"\\nResult df:\\n{result_df}\\n\")\n",
    "    print(f\"Best ml model:\\n{best_ml_model}\\n\")\n",
    "    # print(f\"Data Dictionary:\\n{data_dict}\\n\")\n",
    "\n",
    "    if task_type == 'regression':\n",
    "        result_df_reg, best_ml_model_reg = result_df, best_ml_model\n",
    "        \n",
    "    elif task_type == 'classification':\n",
    "        result_df_clf, best_ml_model_clf = result_df, best_ml_model\n",
    "\n",
    "    elif task_type == 'clustering':\n",
    "        result_df_cls, best_ml_model_cls = result_df, best_ml_model\n",
    "\n",
    "result_df_list = [result_df_reg, result_df_clf, result_df_cls]\n",
    "best_ml_model_list = [best_ml_model_reg, best_ml_model_clf, best_ml_model_cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metaheuristic Optimizer (Name)</th>\n",
       "      <th>Metaheuristic Optimizer (Object)</th>\n",
       "      <th>Machine Learning Model (object)</th>\n",
       "      <th>Mean Squared Error (MSE)</th>\n",
       "      <th>Root Mean Squared Error (RMSE)</th>\n",
       "      <th>Mean Absolute Error (MAE)</th>\n",
       "      <th>Mean Absolute Percentage Error (MAPE)</th>\n",
       "      <th>R-Squared</th>\n",
       "      <th>Adjusted R-Squared</th>\n",
       "      <th>Explained Variance Score</th>\n",
       "      <th>Time taken (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OriginalACOR</td>\n",
       "      <td>OriginalACOR(epoch=10, pop_size=10, sample_cou...</td>\n",
       "      <td>(DecisionTreeRegressor(ccp_alpha=100.0, criter...</td>\n",
       "      <td>1.711097e+07</td>\n",
       "      <td>4136.540802</td>\n",
       "      <td>2431.619756</td>\n",
       "      <td>0.287244</td>\n",
       "      <td>0.642126</td>\n",
       "      <td>0.632871</td>\n",
       "      <td>0.642166</td>\n",
       "      <td>60.938422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BaseGA</td>\n",
       "      <td>BaseGA(epoch=10, pop_size=10, pc=0.9, pm=0.05)</td>\n",
       "      <td>(DecisionTreeRegressor(ccp_alpha=81.6666731726...</td>\n",
       "      <td>3.598722e+07</td>\n",
       "      <td>5998.935225</td>\n",
       "      <td>4744.463030</td>\n",
       "      <td>0.727360</td>\n",
       "      <td>0.247332</td>\n",
       "      <td>0.227866</td>\n",
       "      <td>0.252020</td>\n",
       "      <td>8.276748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OriginalPSO</td>\n",
       "      <td>OriginalPSO(epoch=10, pop_size=10, c1=2.05, c2...</td>\n",
       "      <td>(DecisionTreeRegressor(ccp_alpha=50.7209742679...</td>\n",
       "      <td>1.761852e+07</td>\n",
       "      <td>4197.442347</td>\n",
       "      <td>2585.104481</td>\n",
       "      <td>0.306436</td>\n",
       "      <td>0.631511</td>\n",
       "      <td>0.621981</td>\n",
       "      <td>0.631841</td>\n",
       "      <td>13.190174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OriginalSA</td>\n",
       "      <td>OriginalSA(epoch=10, temp_init=100.0, step_siz...</td>\n",
       "      <td>(DecisionTreeRegressor(ccp_alpha=38.2543403005...</td>\n",
       "      <td>3.882667e+07</td>\n",
       "      <td>6231.105382</td>\n",
       "      <td>4959.739063</td>\n",
       "      <td>0.809481</td>\n",
       "      <td>0.187945</td>\n",
       "      <td>0.166943</td>\n",
       "      <td>0.190808</td>\n",
       "      <td>1.031757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Metaheuristic Optimizer (Name)  ... Time taken (s)\n",
       "0                   OriginalACOR  ...      60.938422\n",
       "1                         BaseGA  ...       8.276748\n",
       "2                    OriginalPSO  ...      13.190174\n",
       "3                     OriginalSA  ...       1.031757\n",
       "\n",
       "[4 rows x 11 columns]"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metaheuristic Optimizer (Name)</th>\n",
       "      <th>Metaheuristic Optimizer (Object)</th>\n",
       "      <th>Machine Learning Model (object)</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Time taken (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OriginalACOR</td>\n",
       "      <td>OriginalACOR(epoch=10, pop_size=10, sample_cou...</td>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=53.033442368...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>25.048588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BaseGA</td>\n",
       "      <td>BaseGA(epoch=10, pop_size=10, pc=0.9, pm=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=10.898975267...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5.153948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OriginalPSO</td>\n",
       "      <td>OriginalPSO(epoch=10, pop_size=10, c1=2.05, c2...</td>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=44.243601216...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>13.709722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OriginalSA</td>\n",
       "      <td>OriginalSA(epoch=10, temp_init=100.0, step_siz...</td>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=63.210560660...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>2.625116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Metaheuristic Optimizer (Name)  ... Time taken (s)\n",
       "0                   OriginalACOR  ...      25.048588\n",
       "1                         BaseGA  ...       5.153948\n",
       "2                    OriginalPSO  ...      13.709722\n",
       "3                     OriginalSA  ...       2.625116\n",
       "\n",
       "[4 rows x 8 columns]"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metaheuristic Optimizer (Name)</th>\n",
       "      <th>Metaheuristic Optimizer (Object)</th>\n",
       "      <th>Machine Learning Model (object)</th>\n",
       "      <th>Silhouette Score</th>\n",
       "      <th>Davies-Bouldin Index</th>\n",
       "      <th>Calinski-Harabasz Index</th>\n",
       "      <th>Time taken (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OriginalACOR</td>\n",
       "      <td>OriginalACOR(epoch=10, pop_size=10, sample_cou...</td>\n",
       "      <td>KMeans(algorithm='elkan', max_iter=100, n_clus...</td>\n",
       "      <td>0.657036</td>\n",
       "      <td>0.521378</td>\n",
       "      <td>505.386748</td>\n",
       "      <td>22.296300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BaseGA</td>\n",
       "      <td>BaseGA(epoch=10, pop_size=10, pc=0.9, pm=0.05)</td>\n",
       "      <td>KMeans(max_iter=100, n_clusters=14, n_init=10)</td>\n",
       "      <td>0.657036</td>\n",
       "      <td>0.521378</td>\n",
       "      <td>505.386748</td>\n",
       "      <td>6.366074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OriginalPSO</td>\n",
       "      <td>OriginalPSO(epoch=10, pop_size=10, c1=2.05, c2...</td>\n",
       "      <td>KMeans(algorithm='elkan', max_iter=192, n_clus...</td>\n",
       "      <td>0.631917</td>\n",
       "      <td>0.586234</td>\n",
       "      <td>540.691105</td>\n",
       "      <td>4.541927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OriginalSA</td>\n",
       "      <td>OriginalSA(epoch=10, temp_init=100.0, step_siz...</td>\n",
       "      <td>KMeans(max_iter=397, n_clusters=12, n_init=10)</td>\n",
       "      <td>0.639805</td>\n",
       "      <td>0.542976</td>\n",
       "      <td>411.842281</td>\n",
       "      <td>1.092453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Metaheuristic Optimizer (Name)  ... Time taken (s)\n",
       "0                   OriginalACOR  ...      22.296300\n",
       "1                         BaseGA  ...       6.366074\n",
       "2                    OriginalPSO  ...       4.541927\n",
       "3                     OriginalSA  ...       1.092453\n",
       "\n",
       "[4 rows x 7 columns]"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy purps\n",
    "\n",
    "def evaluation_metrics(task_type):\n",
    "    if task_type == 'regression':\n",
    "        \"\"\" Regression \"\"\"\n",
    "\n",
    "        regression_metrics_names = [\"Mean Squared Error (MSE)\",\n",
    "                                    \"Root Mean Squared Error (RMSE)\",\n",
    "                                    \"Mean Absolute Error (MAE)\",\n",
    "                                    \"Mean Absolute Percentage Error (MAPE)\",\n",
    "                                    \"R-Squared\",\n",
    "                                    \"Adjusted R-Squared\",\n",
    "                                    \"Explained Variance Score\",\n",
    "                                    ]\n",
    "\n",
    "        def regression_evaluation_metrics(y_test, y_pred, n, p):\n",
    "            # Calculating metrics\n",
    "            MSE = mean_squared_error(y_test, y_pred)\n",
    "            RMSE = np.sqrt(MSE)\n",
    "            MAE = mean_absolute_error(y_test, y_pred)\n",
    "            MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            R2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Adjusted R-Squared\n",
    "            adj_r2 = 1 - (1 - R2) * ((n - 1) / (n - p - 1))\n",
    "            \n",
    "            # Explained Variance Score\n",
    "            expl_var_score = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "            # Create a list of metric values in the same order as the dictionary keys\n",
    "            metrics_values = [MSE, RMSE, MAE, MAPE, R2, adj_r2, expl_var_score]\n",
    "\n",
    "            # Return all metrics as a tuple\n",
    "            return metrics_values\n",
    "\n",
    "        return [regression_metrics_names, regression_evaluation_metrics]\n",
    "\n",
    "    elif task_type == 'classification':\n",
    "        \"\"\" Classification \"\"\"\n",
    "\n",
    "        classification_metrics_names = [\"Accuracy\",\n",
    "                            \"Precision\",\n",
    "                            \"Recall\",\n",
    "                            \"F1-Score\",\n",
    "                            ]\n",
    "\n",
    "        def classification_evaluation_metrics(y_test, y_pred, n_classes):\n",
    "            # Average method for certain metrics\n",
    "            if n_classes > 2:\n",
    "                average = 'macro'\n",
    "                \n",
    "                precision = precision_score(y_test, y_pred, average=average, zero_division=np.nan)\n",
    "                recall = recall_score(y_test, y_pred, average=average)\n",
    "                f1_sc = f1_score(y_test, y_pred, average=average)\n",
    "\n",
    "            else: # if n_classes == 2:\n",
    "                average = 'binary'\n",
    "                \n",
    "                precision = precision_score(y_test, y_pred, average=average, zero_division=np.nan)\n",
    "                recall = recall_score(y_test, y_pred, average=average)\n",
    "                f1_sc = f1_score(y_test, y_pred, average=average)\n",
    "\n",
    "            # accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            # Create a list of metric values in the same order as the dictionary keys\n",
    "            metrics_values = [accuracy, precision, recall, f1_sc]\n",
    "\n",
    "            return metrics_values\n",
    "\n",
    "        return [classification_metrics_names, classification_evaluation_metrics]\n",
    "\n",
    "    elif task_type == 'clustering':\n",
    "        \"\"\" Clustering \"\"\"\n",
    "\n",
    "        clustering_metrics_names = [\"Silhouette Score\",\n",
    "                                    \"Davies-Bouldin Index\",\n",
    "                                    \"Calinski-Harabasz Index\",\n",
    "                                    ]\n",
    "\n",
    "        def clustering_evaluation_metrics(df, labels):\n",
    "            # Silhouette score\n",
    "            silhouette = silhouette_score(df, labels)       # Closer to 1 values suggest better-defined clusters.\n",
    "            db_index = davies_bouldin_score(df, labels)     # A lower score is preferable\n",
    "            ch_index = calinski_harabasz_score(df, labels)  # Higher is better\n",
    "\n",
    "            # Create a list of metric values in the same order as the dictionary keys\n",
    "            metrics_values = [silhouette, db_index, ch_index]\n",
    "\n",
    "            return metrics_values\n",
    "\n",
    "        return [clustering_metrics_names, clustering_evaluation_metrics]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task_type: {task_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Dependecies handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependencies_handling(self, all_decoded_paras):\n",
    "    # Model Name\n",
    "    ml_model_name = self.model.__class__.__name__\n",
    "\n",
    "    paras_names = list(all_decoded_paras.keys())\n",
    "\n",
    "    if ml_model_name == 'RandomForestRegressor':\n",
    "\n",
    "        required_keys = {\"bootstrap\", \"max_samples\", \"oob_score\"}\n",
    "\n",
    "        if all(key in paras_names for key in required_keys):\n",
    "\n",
    "            # Dep2: Handle the interdependency between bootstrap and max_samples\n",
    "            if not all_decoded_paras[\"bootstrap\"]:\n",
    "                all_decoded_paras[\"max_samples\"] = None  # Ensure max_samples is None if bootstrap=False\n",
    "                all_decoded_paras[\"oob_score\"] = False\n",
    "        \n",
    "        else:\n",
    "            for required_key in required_keys:\n",
    "                all_decoded_paras[required_key] = self.model.get_params()[required_key]\n",
    "\n",
    "    elif ml_model_name == 'RandomForestClassifier':\n",
    "\n",
    "        required_keys = {\"bootstrap\", \"max_samples\", \"oob_score\", \"class_weight\", \"warm_start\", \"monotonic_cst\"}\n",
    "        \n",
    "        if all(key in paras_names for key in required_keys):\n",
    "                \n",
    "            # Dep2: Handle the interdependency between bootstrap and max_samples\n",
    "            if not all_decoded_paras[\"bootstrap\"]:\n",
    "                all_decoded_paras[\"max_samples\"] = None     # Ensure max_samples is None if bootstrap=False\n",
    "                all_decoded_paras[\"oob_score\"] = False\n",
    "\n",
    "            # Dep3: Handle monotonic constraint\n",
    "            if self.n_classes > 2 or self.is_multioutput:\n",
    "                all_decoded_paras[\"monotonic_cst\"] = None   # set monotonic_cst to None for multiclass classification or multi-output\n",
    "            \n",
    "            # Dep4: class_weight & warm_start\n",
    "            if all_decoded_paras[\"class_weight\"] in ('balanced', 'balanced_subsample'):\n",
    "                all_decoded_paras[\"warm_start\"] = False\n",
    "        \n",
    "        else:\n",
    "            for required_key in required_keys:\n",
    "                all_decoded_paras[required_key] = self.model.get_params()[required_key]\n",
    "\n",
    "    return all_decoded_paras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict(data, task_type):\n",
    "\n",
    "    if task_type in ('regression', 'classification'):\n",
    "        data = {\n",
    "            \"X_train\": data[0],\n",
    "            \"X_test\": data[1],\n",
    "            \"y_train\": data[2],\n",
    "            \"y_test\": data[3], \n",
    "        }\n",
    "        \n",
    "    elif task_type == 'clustering':\n",
    "        data = {\"X\" : data}\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dictionary for regression data:\n",
      " {'X_train':           age       sex       bmi  children    smoker    region\n",
      "0    0.347385  1.023333 -0.447469  0.766327 -0.342193  1.374625\n",
      "1    1.058097  1.023333  1.629846 -0.059748 -0.342193  1.374625\n",
      "2    1.768810 -0.977199  1.536196  1.592401 -0.342193  0.467087\n",
      "3    0.631670  1.023333  0.029292  0.766327 -0.342193  1.374625\n",
      "4   -0.079043  1.023333 -1.516776  1.592401 -0.342193  0.467087\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "949  1.200240 -0.977199  2.023174 -0.885822 -0.342193  0.467087\n",
      "950  0.205242  1.023333  1.217788  0.766327 -0.342193  0.467087\n",
      "951  1.129168 -0.977199  0.080373 -0.885822 -0.342193  1.374625\n",
      "952 -0.292256  1.023333 -0.498551 -0.059748 -0.342193  1.374625\n",
      "953  0.134171 -0.977199 -0.336793 -0.059748 -0.342193  0.467087\n",
      "\n",
      "[954 rows x 6 columns], 'X_test':           age       sex       bmi  children    smoker    region\n",
      "0   -0.150114  1.023333  2.809829  1.592401 -0.342193  0.467087\n",
      "1   -1.074040  1.023333  0.613324  2.418476 -0.342193 -1.347988\n",
      "2   -1.429396  1.023333 -0.226116 -0.885822 -0.342193  1.374625\n",
      "3   -0.860826 -0.977199  0.063346  1.592401 -0.342193 -0.440451\n",
      "4   -1.429396  1.023333  0.936840 -0.885822 -0.342193 -0.440451\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "234 -0.789755  1.023333 -1.279247  0.766327 -0.342193 -1.347988\n",
      "235 -1.500467  1.023333 -0.826324 -0.885822  2.922328 -1.347988\n",
      "236  0.631670  1.023333  0.128049  1.592401 -0.342193 -1.347988\n",
      "237 -0.150114 -0.977199 -0.617741 -0.885822  2.922328  0.467087\n",
      "238  1.413453  1.023333 -0.777796 -0.885822 -0.342193 -0.440451\n",
      "\n",
      "[239 rows x 6 columns], 'y_train': 74       7726.8540\n",
      "162     10450.5520\n",
      "603     16085.1275\n",
      "424      8968.3300\n",
      "408      6652.5288\n",
      "           ...    \n",
      "1172    11093.6229\n",
      "1227     7162.0122\n",
      "1266    10704.4700\n",
      "965      4746.3440\n",
      "1262     6770.1925\n",
      "Name: charges, Length: 954, dtype: float64, 'y_test': 660      6435.62370\n",
      "754     17128.42608\n",
      "487      1253.93600\n",
      "429     18804.75240\n",
      "559      1646.42970\n",
      "           ...     \n",
      "813      4428.88785\n",
      "157     15518.18025\n",
      "645     10141.13620\n",
      "750     19539.24300\n",
      "1015    12124.99240\n",
      "Name: charges, Length: 239, dtype: float64} \n",
      "\n",
      "Data dictionary for classification data:\n",
      " {'X_train':      SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "0        -1.721568     -0.324840      -1.347036     -1.320168\n",
      "1        -1.124492     -1.226129       0.414290      0.651867\n",
      "2         1.144395     -0.550162       0.584741      0.257460\n",
      "3        -1.124492      0.125805      -1.290219     -1.451638\n",
      "4        -0.408002     -1.226129       0.130206      0.125991\n",
      "..             ...           ...            ...           ...\n",
      "115      -1.124492      0.125805      -1.290219     -1.451638\n",
      "116      -1.363322      0.351127      -1.403853     -1.320168\n",
      "117      -0.408002      2.604352      -1.347036     -1.320168\n",
      "118       1.263810      0.125805       0.641558      0.388929\n",
      "119      -1.482738      0.125805      -1.290219     -1.320168\n",
      "\n",
      "[120 rows x 4 columns], 'X_test':     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "0       -1.721568     -0.099517      -1.403853     -1.320168\n",
      "1        0.308489     -0.099517       0.641558      0.783336\n",
      "2       -1.124492     -1.451452      -0.267513     -0.268416\n",
      "3       -1.005077     -1.676774      -0.267513     -0.268416\n",
      "4       -1.721568      0.351127      -1.403853     -1.320168\n",
      "5        0.547319      0.576450       0.527924      0.520398\n",
      "6       -1.482738      1.252417      -1.574303     -1.320168\n",
      "7       -0.527417      0.801772      -1.176585     -1.320168\n",
      "8        0.786149     -0.099517       0.812009      1.046275\n",
      "9       -0.527417     -0.099517       0.414290      0.388929\n",
      "10       1.741470     -0.324840       1.436996      0.783336\n",
      "11       1.263810      0.125805       0.755192      1.440682\n",
      "12       0.786149     -0.099517       1.152911      1.309213\n",
      "13       0.666734      0.351127       0.414290      0.388929\n",
      "14      -1.005077      0.801772      -1.290219     -1.320168\n",
      "15      -1.005077      0.576450      -1.347036     -1.320168\n",
      "16      -0.049756      2.153707      -1.460669     -1.320168\n",
      "17      -0.288587     -1.226129       0.073389     -0.136947\n",
      "18       0.308489     -0.324840       0.527924      0.257460\n",
      "19       0.189074     -0.099517       0.584741      0.783336\n",
      "20      -0.527417      1.477740      -1.290219     -1.320168\n",
      "21       1.024980      0.125805       1.039277      1.572151\n",
      "22       0.905565     -0.324840       0.471107      0.125991\n",
      "23       0.308489     -1.000807       1.039277      0.257460\n",
      "24       0.666734     -0.550162       1.039277      1.309213\n",
      "25       1.024980     -0.099517       0.698375      0.651867\n",
      "26       0.905565     -0.099517       0.357473      0.257460\n",
      "27      -0.169171      1.703062      -1.176585     -1.188699\n",
      "28       0.786149     -0.099517       0.982460      0.783336\n",
      "29      -0.766247      0.801772      -1.347036     -1.320168, 'y_train': array([0, 2, 1, 0, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 2, 2, 0, 1, 0,\n",
      "       2, 0, 1, 2, 2, 0, 2, 0, 0, 1, 1, 0, 2, 2, 1, 1, 2, 1, 0, 1, 0, 2,\n",
      "       0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 2, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1,\n",
      "       2, 1, 1, 2, 0, 0, 0, 2, 1, 2, 1, 2, 2, 1, 0, 2, 1, 0, 2, 0, 2, 1,\n",
      "       1, 0, 1, 2, 0, 0, 2, 2, 2, 1, 2, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1,\n",
      "       0, 2, 1, 1, 0, 0, 0, 0, 1, 0]), 'y_test': array([0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2,\n",
      "       1, 2, 2, 1, 1, 0, 2, 0])} \n",
      "\n",
      "Data dictionary for clustering data:\n",
      " {'X':        Gender       Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0    1.141195 -1.425414           -1.779171               -0.435989\n",
      "1    1.141195 -1.282367           -1.779171                1.199413\n",
      "2   -0.876275 -1.353890           -1.739447               -1.720949\n",
      "3   -0.876275 -1.139319           -1.739447                1.043661\n",
      "4   -0.876275 -0.567131           -1.699723               -0.397051\n",
      "..        ...       ...                 ...                     ...\n",
      "193 -0.876275 -0.066466            2.113819                1.588795\n",
      "194 -0.876275  0.577246            2.391890               -1.331567\n",
      "195 -0.876275 -0.281037            2.391890                1.121537\n",
      "196 -0.876275  0.434198            2.630236               -0.864309\n",
      "197  1.141195 -0.495608            2.630236                0.926846\n",
      "\n",
      "[198 rows x 4 columns]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing data_dictionary\n",
    "\n",
    "for task_type, preprocessed_df in zip(task_type_list, preprocessed_df_list):\n",
    "\n",
    "    if task_type == 'regression':\n",
    "        df_reg_dict = create_data_dict(preprocessed_df, task_type)\n",
    "\n",
    "        print(f\"Data dictionary for {task_type} data:\\n\", df_reg_dict,'\\n')\n",
    "\n",
    "    elif task_type == 'classification':\n",
    "        df_clf_dict = create_data_dict(preprocessed_df, task_type)\n",
    "\n",
    "        print(f\"Data dictionary for {task_type} data:\\n\", df_clf_dict,'\\n')\n",
    "\n",
    "    elif task_type == 'clustering':\n",
    "        df_cls_dict = create_data_dict(preprocessed_df, task_type)\n",
    "\n",
    "        print(f\"Data dictionary for {task_type} data:\\n\", df_cls_dict,'\\n')\n",
    "\n",
    "data_dict_list = [df_reg_dict, df_clf_dict, df_cls_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Problem Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedProblem(Problem):\n",
    "    def __init__(\n",
    "                    self,\n",
    "                    bounds=None,\n",
    "                    minmax=\"max\",\n",
    "                    data=None,\n",
    "                    optimizer=None,\n",
    "                    model=None,\n",
    "                    task_type=None,\n",
    "                    eval_metrics=None,\n",
    "                    paras_bounds_names=None,\n",
    "                    n_classes=None,\n",
    "                    is_multioutput=None,\n",
    "                    n_obsv=None,\n",
    "                    n_predictors=None,\n",
    "                    **kwargs\n",
    "                ):\n",
    "        self.data = data\n",
    "        self.optimizer = optimizer        \n",
    "        self.model = model\n",
    "        self.task_type = task_type\n",
    "        self.eval_metrics = eval_metrics\n",
    "\n",
    "        self.best_model = None\n",
    "        self.best_metrics = None\n",
    "        self.all_decoded_paras = {}\n",
    "        self.encoders = {}\n",
    "        \n",
    "        self.paras_bounds_names=paras_bounds_names\n",
    "        self.n_classes = n_classes\n",
    "        self.is_multioutput = is_multioutput\n",
    "        self.n_obsv = n_obsv\n",
    "        self.n_predictors = n_predictors\n",
    "\n",
    "        super().__init__(bounds, minmax, **kwargs)\n",
    "\n",
    "    def obj_func(self, x):\n",
    "        task_type = self.task_type\n",
    "        optimizer = self.optimizer\n",
    "        all_decoded_paras = self.all_decoded_paras\n",
    "\n",
    "        x_decoded = self.decode_solution(x)\n",
    "\n",
    "        # print(self.paras_bounds_names)\n",
    "        for paras_name in self.paras_bounds_names:\n",
    "            all_decoded_paras[paras_name[:-6]] = None if x_decoded[paras_name] == 'none' else x_decoded[paras_name]\n",
    "            # print(all_decoded_paras[paras_name[:-6]])\n",
    "\n",
    "        # Decoded paras (dict) after handling dependecies\n",
    "        all_decoded_paras = dependencies_handling(self, all_decoded_paras)\n",
    "        # print(all_decoded_paras)\n",
    "\n",
    "        # Defining the model and assigning hyperparameters\n",
    "        ml_model = self.model.__class__(**all_decoded_paras)  \n",
    "\n",
    "        if task_type in ('regression', 'classification'):\n",
    "\n",
    "            # Fit the model\n",
    "            ml_model.fit(self.data[\"X_train\"], self.data[\"y_train\"])\n",
    "\n",
    "            # Make the predictions\n",
    "            y_predict = ml_model.predict(self.data[\"X_test\"])\n",
    "\n",
    "            if task_type == 'regression':\n",
    "                metrics = self.eval_metrics(self.data[\"y_test\"], y_predict, self.n_obsv, self.n_predictors)\n",
    "                metric_current_best = metrics[0]\n",
    "\n",
    "            elif task_type == 'classification':\n",
    "                metrics = self.eval_metrics(self.data[\"y_test\"], y_predict, self.n_classes)\n",
    "                metric_current_best = metrics[-1]\n",
    "\n",
    "        elif task_type == 'clustering':\n",
    "            \n",
    "            # Fit the model\n",
    "            ml_model.fit_predict(self.data[\"X\"])\n",
    "            \n",
    "            # Make the predictions\n",
    "            labels = ml_model.fit_predict(self.data[\"X\"])\n",
    "            \n",
    "            metrics = self.eval_metrics(self.data[\"X\"], labels)\n",
    "            metric_current_best = metrics[0]\n",
    "\n",
    "        # if optimizer.g_best is None:\n",
    "        #     print(\"No global best found yet.\")\n",
    "        # elif optimizer.g_best.target is None:\n",
    "        #     print(\"Global best found, but target is not available.\")\n",
    "        # else:\n",
    "\n",
    "        if optimizer.g_best is not None and optimizer.g_best.target is not None:\n",
    "            # Store the model if it's better\n",
    "            global_current_best = optimizer.g_best.target.fitness\n",
    "\n",
    "            if self.minmax == 'min':\n",
    "                metrics_comparison = metric_current_best < global_current_best\n",
    "            else:\n",
    "                metrics_comparison = metric_current_best > global_current_best\n",
    "            \n",
    "            if self.best_model is None or metrics_comparison:\n",
    "                self.best_model = ml_model\n",
    "                self.best_metrics = metrics\n",
    "\n",
    "        return metric_current_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Model and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_and_variables(data, task_type):\n",
    "    model = None\n",
    "    reference_metric = None\n",
    "    n_obsv = None\n",
    "    n_predictors = None\n",
    "    n_classes = None\n",
    "    is_multioutput = None\n",
    "\n",
    "    if task_type == 'regression':\n",
    "        model = RandomForestRegressor()\n",
    "        reference_metric = \"Mean Squared Error (MSE)\"\n",
    "\n",
    "        n_obsv = len(data[\"y_test\"])  # Number of observations\n",
    "        n_predictors = data[\"X_test\"].shape[1]  # Number of predictors (features)\n",
    "\n",
    "    elif task_type == 'classification':\n",
    "        n_classes = len(np.unique(data[\"y_train\"]))\n",
    "        is_multioutput = len(data[\"y_train\"].shape) > 1 and data[\"y_train\"].shape[1] > 1 if data[\"y_train\"] is not None else False\n",
    "\n",
    "        model = RandomForestClassifier()\n",
    "        reference_metric = \"F1-Score\"\n",
    "\n",
    "    elif task_type == 'clustering':\n",
    "        model = KMeans()\n",
    "        reference_metric = \"Silhouette Score\"\n",
    "\n",
    "    return model, reference_metric, n_obsv, n_predictors, n_classes, is_multioutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current task type: regression\n",
      "Model name: RandomForestRegressor\n",
      "Reference metric: Mean Squared Error (MSE)\n",
      "Number of observation: 239\n",
      "Number of predictors: 6\n",
      "Number of classes: None\n",
      "Is it multilabel/multioutput? : None\n",
      "\n",
      "Current task type: classification\n",
      "Model name: RandomForestClassifier\n",
      "Reference metric: F1-Score\n",
      "Number of observation: None\n",
      "Number of predictors: None\n",
      "Number of classes: 3\n",
      "Is it multilabel/multioutput? : False\n",
      "\n",
      "Current task type: clustering\n",
      "Model name: KMeans\n",
      "Reference metric: Silhouette Score\n",
      "Number of observation: None\n",
      "Number of predictors: None\n",
      "Number of classes: None\n",
      "Is it multilabel/multioutput? : None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing Model and Variable\n",
    "\n",
    "for task_type, data_dict in zip(task_type_list, data_dict_list):\n",
    "\n",
    "    model, reference_metric, n_obsv, n_predictors, n_classes, is_multioutput = model_and_variables(data_dict, task_type)\n",
    "\n",
    "    print(f\"Current task type: {task_type}\")\n",
    "    print(f\"Model name: {model.__class__.__name__}\")\n",
    "    print(f\"Reference metric: {reference_metric}\")\n",
    "\n",
    "    print(f\"Number of observation: {n_obsv}\")\n",
    "    print(f\"Number of predictors: {n_predictors}\")\n",
    "\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "    print(f\"Is it multilabel/multioutput? : {is_multioutput}\\n\")\n",
    "\n",
    "    # if task_type == 'regression':\n",
    "    #     print(f\"Number of observation: {n_obsv}\")\n",
    "    #     print(f\"Number of predictors: {n_predictors}\")\n",
    "\n",
    "    # elif task_type == 'classification':\n",
    "    #     print(f\"Number of classes: {n_classes}\")\n",
    "    #     print(f\"Is it multilabel/multioutput? : {is_multioutput}\")\n",
    "\n",
    "    if task_type == 'regression':\n",
    "        model_var_reg = [model, reference_metric, n_obsv, n_predictors, n_classes, is_multioutput]\n",
    "\n",
    "    elif task_type == 'classification':\n",
    "        model_var_clf = [model, reference_metric, n_obsv, n_predictors, n_classes, is_multioutput]\n",
    "\n",
    "    elif task_type == 'clustering':\n",
    "        model_var_cls = [model, reference_metric, n_obsv, n_predictors, n_classes, is_multioutput]\n",
    "\n",
    "model_var_list = [model_var_reg, model_var_clf, model_var_cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Hyperparameter Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters_bounds(model, random_state=42):\n",
    "    # Model Name\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    if model_name == 'RandomForestRegressor':\n",
    "        paras_bounds = [\n",
    "            IntegerVar(lb=1, ub=100, name=\"n_estimators_paras\"),\n",
    "            StringVar(valid_sets=('squared_error', 'absolute_error', 'friedman_mse', 'poisson'), name=\"criterion_paras\"),\n",
    "            MixedSetVar(valid_sets=('none', 10, 50, 100), name=\"max_depth_paras\"),\n",
    "            IntegerVar(lb=2, ub=100, name=\"min_samples_split_paras\"),                     # int in the range [2, inf) or a float in the range (0.0, 1.0]\n",
    "            IntegerVar(lb=2, ub=100, name=\"min_samples_leaf_paras\"),                      # int in the range [1, inf) or a float in the range (0.0, 1.0)\n",
    "            FloatVar(lb=0., ub=0.5, name=\"min_weight_fraction_leaf_paras\"),             # float in the range [0.0, 0.5]\n",
    "            MixedSetVar(valid_sets=('none', 'sqrt', 'log2', 1, 5, 10, 50, 100), name=\"max_features_paras\"),\n",
    "            IntegerVar(lb=2, ub=100, name=\"max_leaf_nodes_paras\"),                      # int in the range [2, inf)\n",
    "            FloatVar(lb=1., ub=100., name=\"min_impurity_decrease_paras\"),\n",
    "            BoolVar(n_vars=1, name=\"bootstrap_paras\"),                                  # `max_sample` cannot be set if `bootstrap=False`. Either switch to `bootstrap=True` or set `max_sample=None`\n",
    "            BoolVar(n_vars=1, name=\"oob_score_paras\"),                                  # Only available if bootstrap=True\n",
    "            MixedSetVar(valid_sets=('none', 10, 50, 100), name=\"n_jobs_paras\"),\n",
    "            MixedSetVar(valid_sets=('none', random_state), name=\"random_state_paras\"),  # Dependant towards bootstrap=True\n",
    "            BoolVar(n_vars=1, name=\"warm_start_paras\"),\n",
    "            FloatVar(lb=0., ub=100., name=\"ccp_alpha_paras\"),\n",
    "            MixedSetVar(valid_sets=('none', 5, 10, 15), name=\"max_samples_paras\"),      # `max_sample` cannot be set if `bootstrap=False`. Either switch to `bootstrap=True` or set `max_sample=None`\n",
    "\n",
    "            # MixedSetVar(valid_sets=('none', -1, 0, -1), name=\"monotonic_cst_paras\"),    # unspported when n_outputs_ > 1 (multioutput regression) or data has missing (NA) values\n",
    "            # IntegerVar(lb=0, ub=3, name=\"verbose_paras\"),                             # Irrelevant\n",
    "        ]\n",
    "\n",
    "    elif model_name == 'RandomForestClassifier':\n",
    "        paras_bounds = [\n",
    "            IntegerVar(lb=1, ub=100, name=\"n_estimators_paras\"),\n",
    "            StringVar(valid_sets=('gini', 'entropy', 'log_loss'), name=\"criterion_paras\"),\n",
    "            MixedSetVar(valid_sets=('none', 10, 50, 100), name=\"max_depth_paras\"),\n",
    "            IntegerVar(lb=2, ub=100, name=\"min_samples_split_paras\"),                     # int in the range [2, inf) or a float in the range (0.0, 1.0]\n",
    "            IntegerVar(lb=2, ub=100, name=\"min_samples_leaf_paras\"),                      # int in the range [1, inf) or a float in the range (0.0, 1.0)\n",
    "            FloatVar(lb=0., ub=0.5, name=\"min_weight_fraction_leaf_paras\"),             # float in the range [0.0, 0.5]\n",
    "            MixedSetVar(valid_sets=('none', 'sqrt', 'log2', 1, 5, 10, 50, 100), name=\"max_features_paras\"),\n",
    "            IntegerVar(lb=2, ub=100, name=\"max_leaf_nodes_paras\"),                      # int in the range [2, inf)\n",
    "            FloatVar(lb=1., ub=100., name=\"min_impurity_decrease_paras\"),\n",
    "            # BoolVar(n_vars=1, name=\"bootstrap_paras\"),                                  # `max_sample` cannot be set if `bootstrap=False`. Either switch to `bootstrap=True` or set `max_sample=None`\n",
    "            BoolVar(n_vars=1, name=\"oob_score_paras\"),                                  # Only available if bootstrap=True\n",
    "            MixedSetVar(valid_sets=('none', 10, 50, 100), name=\"n_jobs_paras\"),\n",
    "            MixedSetVar(valid_sets=('none', random_state), name=\"random_state_paras\"),  # Dependant towards bootstrap=True\n",
    "            BoolVar(n_vars=1, name=\"warm_start_paras\"),\n",
    "            MixedSetVar(valid_sets=('none', 'balanced', 'balanced_subsample'), name=\"class_weight_paras\"),\n",
    "            FloatVar(lb=0., ub=100., name=\"ccp_alpha_paras\"),\n",
    "            MixedSetVar(valid_sets=('none', 5, 10, 15), name=\"max_samples_paras\"),      # `max_sample` cannot be set if `bootstrap=False`. Either switch to `bootstrap=True` or set `max_sample=None`\n",
    "            MixedSetVar(valid_sets=('none', -1, 0, 1), name=\"monotonic_cst_paras\")      # not supported when n_classes > 2 (multiclass clf), n_outputs_ > 1 (multi-output), or data has missing values\n",
    "\n",
    "            # IntegerVar(lb=0, ub=3, name=\"verbose_paras\"),                             # Irrelevant\n",
    "        ]\n",
    "\n",
    "    elif model_name == 'KMeans':\n",
    "        paras_bounds = [\n",
    "            # FloatVar(lb=1e-5, ub=1e3, name=\"tol_paras\"),\n",
    "            # StringVar(valid_sets=('linear', 'poly', 'rbf', 'sigmoid'), name=\"kernel_paras\"),\n",
    "            StringVar(valid_sets=('lloyd', 'elkan'), name=\"algorithm_paras\"),\n",
    "            IntegerVar(lb=2, ub=20, name=\"n_clusters_paras\"),\n",
    "            IntegerVar(lb=100, ub=500, name=\"max_iter_paras\"),\n",
    "            MixedSetVar(valid_sets=('auto', 1, 5, 10, 15, 20), name=\"n_init_paras\"),\n",
    "            # BoolVar(n_vars=1, name=\"probability_paras\"),\n",
    "        ]\n",
    "\n",
    "    paras_bounds_names = [] # List containing names of the hyperparameters\n",
    "    for i, _ in enumerate(paras_bounds):\n",
    "        paras_bounds_names.append(paras_bounds[i].name)  # Store each of parameter name (w/ \"\")\n",
    "\n",
    "    return paras_bounds, paras_bounds_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model: RandomForestRegressor\n",
      "Paras bounds: [<mealpy.utils.space.IntegerVar object at 0x000002D3788A00D0>, <mealpy.utils.space.StringVar object at 0x000002D37D8C0FD0>, <mealpy.utils.space.MixedSetVar object at 0x000002D37D8C12D0>, <mealpy.utils.space.FloatVar object at 0x000002D37D8C2750>, <mealpy.utils.space.FloatVar object at 0x000002D37D8C0190>, <mealpy.utils.space.FloatVar object at 0x000002D37D8C1810>, <mealpy.utils.space.MixedSetVar object at 0x000002D37D8C3E10>, <mealpy.utils.space.IntegerVar object at 0x000002D37D8C0E90>, <mealpy.utils.space.FloatVar object at 0x000002D37D8C21D0>, <mealpy.utils.space.BoolVar object at 0x000002D37D8C1650>, <mealpy.utils.space.BoolVar object at 0x000002D37D8C1890>, <mealpy.utils.space.MixedSetVar object at 0x000002D37D8C2910>, <mealpy.utils.space.MixedSetVar object at 0x000002D37D8C1BD0>, <mealpy.utils.space.BoolVar object at 0x000002D37D8C2F10>, <mealpy.utils.space.FloatVar object at 0x000002D37D8C0710>, <mealpy.utils.space.MixedSetVar object at 0x000002D37D8C0250>]\n",
      "Paras bounds names: ['n_estimators_paras', 'criterion_paras', 'max_depth_paras', 'min_samples_split_paras', 'min_samples_leaf_paras', 'min_weight_fraction_leaf_paras', 'max_features_paras', 'max_leaf_nodes_paras', 'min_impurity_decrease_paras', 'bootstrap_paras', 'oob_score_paras', 'n_jobs_paras', 'random_state_paras', 'warm_start_paras', 'ccp_alpha_paras', 'max_samples_paras']\n",
      "\n",
      "Current model: RandomForestClassifier\n",
      "Paras bounds: [<mealpy.utils.space.IntegerVar object at 0x000002D37DF30650>, <mealpy.utils.space.StringVar object at 0x000002D37A4D1B50>, <mealpy.utils.space.MixedSetVar object at 0x000002D37A4D0C50>, <mealpy.utils.space.IntegerVar object at 0x000002D37D8C1510>, <mealpy.utils.space.IntegerVar object at 0x000002D37D8C2610>, <mealpy.utils.space.FloatVar object at 0x000002D37DB92150>, <mealpy.utils.space.MixedSetVar object at 0x000002D37DAE8950>, <mealpy.utils.space.IntegerVar object at 0x000002D37DAE8450>, <mealpy.utils.space.FloatVar object at 0x000002D37DAEBB90>, <mealpy.utils.space.BoolVar object at 0x000002D37DAE8B50>, <mealpy.utils.space.BoolVar object at 0x000002D37DAEBA50>, <mealpy.utils.space.MixedSetVar object at 0x000002D37DAEA4D0>, <mealpy.utils.space.MixedSetVar object at 0x000002D37DAEA850>, <mealpy.utils.space.BoolVar object at 0x000002D37DAE9750>, <mealpy.utils.space.MixedSetVar object at 0x000002D37DAEBBD0>, <mealpy.utils.space.FloatVar object at 0x000002D37DAE84D0>, <mealpy.utils.space.MixedSetVar object at 0x000002D37DAE9350>, <mealpy.utils.space.MixedSetVar object at 0x000002D37DAEA1D0>]\n",
      "Paras bounds names: ['n_estimators_paras', 'criterion_paras', 'max_depth_paras', 'min_samples_split_paras', 'min_samples_leaf_paras', 'min_weight_fraction_leaf_paras', 'max_features_paras', 'max_leaf_nodes_paras', 'min_impurity_decrease_paras', 'bootstrap_paras', 'oob_score_paras', 'n_jobs_paras', 'random_state_paras', 'warm_start_paras', 'class_weight_paras', 'ccp_alpha_paras', 'max_samples_paras', 'monotonic_cst_paras']\n",
      "\n",
      "Current model: KMeans\n",
      "Paras bounds: [<mealpy.utils.space.StringVar object at 0x000002D3788A00D0>, <mealpy.utils.space.IntegerVar object at 0x000002D37D8C0FD0>, <mealpy.utils.space.IntegerVar object at 0x000002D37D8C12D0>, <mealpy.utils.space.MixedSetVar object at 0x000002D37D8C1A10>]\n",
      "Paras bounds names: ['algorithm_paras', 'n_clusters_paras', 'max_iter_paras', 'n_init_paras']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing Hyperparameter Bounds\n",
    "for model_var in model_var_list:\n",
    "    \n",
    "    model = model_var[0]\n",
    "\n",
    "    paras_bounds, paras_bounds_names = hyperparameters_bounds(model)\n",
    "\n",
    "    print(f\"Current model: {model.__class__.__name__}\")\n",
    "    print(f\"Paras bounds: {paras_bounds}\")\n",
    "    print(f\"Paras bounds names: {paras_bounds_names}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VI. Final Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def compute_feature_importance(best_ml_model, data_dict, label=None):\n",
    "\n",
    "    if hasattr(best_ml_model, \"coef_\"):  # Linear models\n",
    "        feature_importance = np.abs(best_ml_model.coef_[0])\n",
    "\n",
    "    elif hasattr(best_ml_model, \"feature_importances_\"):  # Tree-based models\n",
    "        feature_importance = best_ml_model.feature_importances_\n",
    "\n",
    "    else:  # Model-agnostic\n",
    "\n",
    "        data = data_dict[\"X\"]\n",
    "\n",
    "        label = \"Cluster\"\n",
    "\n",
    "        data[label] = best_ml_model.labels_\n",
    "        \n",
    "        # Feature-target split\n",
    "        X = data.drop(columns=label)\n",
    "        y = data[label]\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)#, stratify=y)\n",
    "\n",
    "        # Train the model (Random Forest)\n",
    "        fi_model = RandomForestClassifier(random_state=42)\n",
    "        fi_model.fit(X_train, y_train)\n",
    "\n",
    "        # Compute permutation importance\n",
    "        perm_importance = permutation_importance(\n",
    "            fi_model, X_test, y_test, scoring='accuracy', random_state=42\n",
    "        )\n",
    "\n",
    "        # Extract importance scores\n",
    "        importance_df = pd.DataFrame(\n",
    "            {\n",
    "                \"Feature\": X.columns,\n",
    "                \"Importance Mean\": perm_importance.importances_mean,\n",
    "                \"Importance Std\": perm_importance.importances_std,\n",
    "            }\n",
    "        ).sort_values(by=\"Importance Mean\", ascending=False)\n",
    "        \n",
    "        return importance_df\n",
    "\n",
    "    # Create a DataFrame to visualize the importance of each feature\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': data_dict[\"X_train\"].columns,\n",
    "        'Importance': feature_importance\n",
    "    })\n",
    "\n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "    Feature  Importance\n",
      "0       age    0.422060\n",
      "1       sex    0.000068\n",
      "2       bmi    0.013366\n",
      "3  children    0.025532\n",
      "4    smoker    0.532610\n",
      "5    region    0.006364\n",
      "\n",
      "Feature Importance:\n",
      "         Feature  Importance\n",
      "0  SepalLengthCm         0.0\n",
      "1   SepalWidthCm         0.0\n",
      "2  PetalLengthCm         0.0\n",
      "3   PetalWidthCm         0.0\n",
      "\n",
      "Feature Importance:\n",
      "                  Feature  Importance Mean  Importance Std\n",
      "3  Spending Score (1-100)            0.405        0.043012\n",
      "0                  Gender            0.375        0.065192\n",
      "2      Annual Income (k$)            0.360        0.068191\n",
      "1                     Age            0.265        0.012247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test compute_feature_importance\n",
    "\n",
    "for best_ml_model, data_dict in zip(best_ml_model_list, data_dict_list):\n",
    "\n",
    "    print(f\"Feature Importance:\\n{compute_feature_importance(best_ml_model, data_dict)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_visualization(data):\n",
    "    a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
